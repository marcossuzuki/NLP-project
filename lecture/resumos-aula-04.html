<html><head><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"></head><body><h3>Resumos de aula: PLN - 2018</h3><br> <i>Nota: Arquivo gerado por um programa.</i><br><p><table border=1> <tr> <td>Nome</td> <td>Resumo</td> </tr>

<tr>
<td bgcolor=#33cc33>Bruno Aristimunha Pinto</td>
<td> Iniciamos a aula recapitulando os slides oriundos da aula 02, discutimos o artigo publicado na revista Scientometrics para identificação de pessoas importantes apenas com expressão regular. Para tanto, o autor do artigo de “Extract and quantifying eponyms in full-text articles” utilizou como ferramenta apenas expressões regulares em uma abordagem semi-supervisionada com 821 artigos da própria revista. O modelo desenvolvido apresenta quatro etapas para identificação, sendo elas: parte da extração do autor, da referência, do "adjectival eponyms" e do objeto. A simplicidade do código espanta, e sua robustez também com um R² de 0,95, excelente pela simplicidade e pela área. Começamos levantando a importância da recuperação de informação com quaisquer formas escritas, e.g., uma busca no google: “controladores digitais” deve retornar algo que contenha também “controle digital”. Isso indica uma necessidade de um processo de normalização. Na língua portuguesa em específico possuímos diferentes palavras flexionadas em gênero, número ou grau, além de inúmeros inflexões temporais. Como vimos na aula passada, no processo de normalização de palavras possuímos dois tipos técnicas, sendo elas: Stemming e Lemmatization. A diferença fundamental entre elas consiste em que uma realiza um corte, e a outra uma redução considerando sua gramática, Stemming e Lemmatization. Um ponto de diferenciação importante consiste do fato de que um método busca stems (parte de uma palavra), enquanto outro tipo busca um lema (forma básica da palavra). No processo de Stemming a redução geralmente gerar palavras que não possuem significado concreto e servem muito mais como um signo de um conjunto de palavras. Um exemplo pode ser reduzir o conjunto {gatos, gatas, gato e gata}, o resultado esperado possível seria “gat”. Seguindo em frente, saindo categoria para os algoritmos temos o de Lovins, uma das pioneiras da área. Essa linguista desenvolveu esse método durante o seu doutorado nos anos 60. Basicamente, o método consiste de duas simples etapas por palavra, 1ª etapa: procure pelo sufixo de maior tamanho na palavra que atenda o condicional do sufixo, se atende, realize um corte, senão, reduza o sufixo; repita até atender alguma condição ou chegar no comprimento 0. 2ª etapa: aplique as regras que atendam o condicional para transformar a palavra; faça isso em todas as regras possíveis. Essa lista de sufixo possui 294 situações chave e 29 condições para ser atendida. Já a lista de regras é de comprimento 39. Após explicação do método, realizamos uma atividade prática em papel para testarmos os nossos conhecimentos. Outro algoritmo também conhecido e de funcionamento semelhante é o algoritmo de Porter, esse método usa a estrutura de regras e operações de validação, semelhante a busca gulosa presente no algoritmo de Lovins, aqui temos o casamento do maior sufixo possível também. O método possui cinco etapas, a primeira responsável pelos sufixos de inflexão, a segunda, terceira e quarta etapas são responsáveis pelos stemming de sufixos e a última etapa a recodificação. Detalhes e implementações encontram-se disponíveis nesse link (https://tartarus.org/martin/PorterStemmer/), é importante ressaltar que o próprio autor disponibiliza e centraliza as implementações. Observamos que esses algoritmos para Stemming possuem uma forte dependência da língua, em específico da sua estrutura gramatical. Um exemplo brasileiro desenvolvido fora do país é algoritmo o Orengo de 2001, um stemming para língua portuguesa. Esse código é o primeiro para língua portuguesa e possui 199 regras e 8 passos. Algumas considerações sobre o uso da técnica das devem ser feitas. Um ponto também não explorado extensamente na literatura é a eliminação de prefixo, não há nenhum ponto teórico que impeça esse tipo de recorte. Um exemplo que podemos citar é o algoritmo de Affix Stripping Stemmer. Uma forma de verificar o stemming é analisar através do dicionário, e um cálculo relacionado à acurácia e outros métricas. Deve tomar cuidado para não ocorrer situações como under stemming e over stemming. Essas situações análogas aos dos algoritmos de aprendizado de máquina dizem respeito à generalização e especialização do método para o corpus. No over stemming, vamos supor que temos uma palavra X e essa palavra X foi reduzida à um stem que não corresponde ao seu stem correto, quando isso ocorre temos um falso positivo. Do outro lado, no under stemming há duas palavras X1 e X2 com stems que representam signos diferentes, se ambos são classificados na mesma categoria, temos que um falso negativo. Reconhecer quando os métodos erram e o tipo de erro é fundamental no desenvolvimento de uma aplicação. Geralmente, os métodos de stemming tendem a reduzir um tipo de erro e aumentar outro. Dentro de sala, vimos métodos baseados exclusivamente dentro de regras, no entanto, na categorização temos também métodos mais robustos baseado em modelagem estatística. Nos algoritmos estatísticos recorremos à um grande corpus para aprender a morfologia da língua. O ganho do Stemming, comparando os métodos de Stemming e Lemmatization é o tempo computacional, pois, por se tratar que número limitado de complexidade ao analisarmos a complexidade temos um O(n) ou próximo, enquanto o Lemmatization depende de mais busca. As principais desvantagens desses métodos, quando não estatístico, é a forte dependência nas regras. Novas palavras tendem a não se sair bem ao serem reduzidas, por exemplo, se tentar reduzir IPads e IPad, temos um problema. Temos aqui uma limitação de generalização na idade das palavras usadas no texto. Outro problema, mas dessa vez acadêmico, ocorre que na área de processamento natural de linguagem nas etapas de normalização são dificilmente bem descritas, levando à grandes problemas de reprodução (Fokkens et al., 2013). Um link interessante com implementações a serem usadas em futuras aulas é o recurso Python - NLTK ( http://text-processing.com/demo/stem/ ) Referência: FOKKENS, Antske et al. Offspring from reproduction problems: What replication failure teaches us. In: Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2013. p. 1691-1701.</td>
</tr>

<tr>
<td bgcolor=#33cc33>Paulo Ricardo Cunha da Silva</td>
<td> Nessa aula foi exposto os conceitos de Stemming e Lemmanization que pertencem a formas de normalização de texto. Por razões gramaticais, os documentos vão usar diferentes formas de uma palavra, como organiza, organiza e organiza. Além disso, existem famílias de palavras derivadas relacionadas com significados semelhantes, como democracia, democracia e democratização. Em muitas situações, parece que seria útil para uma busca por uma dessas palavras retornar documentos que contenham outra palavra no conjunto. O objetivo de ambos e de lema é reduzir formas inflexórias e, às vezes, formas relacionadas a derivação de uma palavra para uma base comum. Por exemplo:  Estou, são, é =&gt; ser carro, carros, carros, carros =&gt; carro  O resultado desse mapeamento de texto será algo como:  Os carros do menino são cores diferentes =&gt; o carro do garoto é de cor diferente  No entanto, as duas palavras diferem em sua primitiva. Âncora Stemming geralmente se refere a um processo heurístico bruto que corta as extremidades das palavras com a esperança de alcançar este objetivo corretamente a maior parte do tempo, e muitas vezes inclui a remoção de afiamentos derivativos. Âcora Lemmatização geralmente se refere a fazer as coisas corretamente com o uso de um vocabulário e análise morfológica de palavras, normalmente com o objetivo de remover apenas finais de inflexão e retornar a base ou a forma de dicionário de uma palavra, que é conhecida como o lema Âncora. Se confrontado com a serra de token, o retardo pode retornar apenas s, enquanto a lemmatização tentaria retornar ver ou vir, dependendo se o uso do token era como um verbo ou um substantivo. Os dois também podem diferir em que a maioria das vezes colapsa palavras relacionadas derivadamente, enquanto que a lema geralmente apenas colapsa as diferentes formas de inflexão de um lema. O processamento lingüístico para derrubar ou lematizar é muitas vezes feito por um componente de plug-in adicional ao processo de indexação, e existem vários desses componentes, tanto comerciais como de código aberto. Ânnora O algoritmo mais comum para frustrar o inglês, e um que repetidamente demonstrou ser empiricamente muito efetivo, é o algoritmo de Âncora Porter (Porter, 1980). Todo o algoritmo é muito longo e intrincado para apresentar aqui, mas indicaremos sua natureza geral. O algoritmo de Porter consiste em 5 fases de reduções de palavras, aplicadas sequencialmente. No entanto, a forma talhada exata não importa, apenas as classes de equivalência que ela forma. Ao invés de usar um stemmer, você pode usar um lemmatizador, uma ferramenta de PLN que faz análises morfológicas completas para identificar com precisão o lema de cada palavra. Fazer análises morfológicas completas produz, no máximo, benefícios muito modestos para recuperação. É difícil dizer mais, porque qualquer uma das formas de normalização tende a não melhorar o desempenho de recuperação de informações em inglês em conjunto - pelo menos não por muito. Embora ajude muito para algumas consultas, também dói muito o desempenho para os outros. O lançamento aumenta o recall ao mesmo tempo que prejudica a precisão. Como um exemplo do que pode dar errado, observe que o porteter stemmer deriva de todas as seguintes palavras: "operar operacional opera operação operativa operacional operacional" para operar. No entanto, uma vez que opera em suas várias formas é um verbo comum, esperamos perder uma precisão considerável em consultas, como as seguintes com o carregamento de Porter:  operacional e de pesquisa operação e sistema operatório e odontológico  Para um caso como este, mover-se para usar um lemmatizador não resolveria completamente o problema, porque formas particulares de inflexão são usadas em colocações particulares: uma frase com as palavras operam e o sistema não é uma boa combinação para o funcionamento e sistema da consulta. Obter melhor valor da normalização do termo depende mais de questões pragmáticas de uso de palavras do que em questões formais de morfologia lingüística.Cokmo os algoritmos foram baseados no idioma inglês  foi necessário uma construção para o Português e é ai que entra o Algoritmo de Orengo.</td>
</tr>

<tr>
<td bgcolor=#99e699>Raphael Kiyoshi Fukushima</td>
<td> Complementando a aula passada, será um pouco mais aprofundado na parte de normalização de palavras, por ter um potencial como em sistemas de buscas que deveria ser capaz de encontrar documentos com diferentes nomes assim aumentando o alcance de buscas e dando mais possibilidades de encontrar o que o usuário busca. Das técnicas vistas, iremos falar mais sobre o stemming, que se refere a um sistema de heurística que corta palavras, normalmente sufixos e as altera para uma forma mais básica da palavra, mas não necessariamente sua raiz. Temos dois algoritmos mais famosos, para o idioma inglês, o de Lovins (1968) e de Porter (1980), sendo o primeiro mais importante historicamente, pois foi o pioneiro entre os stemmers (o código de stemming) e a tese de doutorado de sua criadora, Julie Beth Lovins, composto de apenas 2 fases, na qual a primeira corta o sufixo baseando-se em 294 sufixos contendo 29 condições (é uma busca gulosa que procura pelo maior sufixo e o remove, se as condições forem cumpridas), e depois 34 regras que podem, ou não alterar o fim da palavra para gerar uma palavra mais "genérica". Agora o algoritmo de Porter foi criado a para um relatório de recuperação de informação por um bibliotecário, Martin F. Porter, sendo mais completo e mais "simples" que o de Lovins é também o mais famoso e usado até mesmo hoje em dia, composto de 5 etapas, em resumo ele é bem parecido com o de Lovins, casa o maior sufixo, remove, procura se é necessário alguma alteração no final da palavra e checa se há alguma vogal restante na palavra, o código de Porter (atualizado) e todas as suas implementações em linguagens de programação estão disponíveis no site do próprio autor: https://tartarus.org/martin/PorterStemmer Há uma versão de stemming para o Português criado por Viviane Moreira Orengo, composto por 199 regras e 8 passos, primeiro remove plural, remove modificação para o feminino, depois remove todo e qualquer alteração de aumentativo, advérbio, substantivo e verbo e no final, remove os acentos (essa parte é meio controversa e pode ser considerada desnecessária/precisa ser alterada). Considerações finais Por que não usar um dicionário para o stemming? Porque faz com que o algoritmo consuma muito mais tempo, e todo o esforço e tempo a mais investido pode não ter um retorno que valha a pena. Stemming para prefixos? Não há nenhum motivo teórico para não fazê-lo, mas ninguém ainda o fez. Stemming perde detalhe e informação? Talvez, um algoritmo bem codificado não deveria permitir perda de informações, mas alguns detalhes podem ser perdidos. Stemming para nomes? Possível Projeto Final?</td>
</tr>

<tr>
<td bgcolor=#99e699>Clarissa Simoyama David</td>
<td> Nesta aula vimos mais a fundo sobre a parte de normalização de texto stemming. Relembrando, no idioma português diversas palavras são flexionadas de acordo com gênero, número ou grau, além de diversos tempos verbais. A normalização de palavras consiste em reduzir ou simplificar palavras, ou até mesmo retirar sufixos ao ponto de deixar a palavra em seu radical. O processo de stemming reduz a palavra até a sua raiz, por exemplo gat pode representar gato, gata, gatos, gatas, referindo-se à um processo de heurística que reduz as extremidades das palavras inclui a remoção de afixos derivacionais, sendo representado por um conjunto de regras que dependem da linguagem. Em suma, stemming é a ação de reduzir em stems e lemmatization é a ação de reduzir em lemmas. Existem diversos algortimos de stemming, principalmente a linguagem inglesa, como sample text, lovins stemmer e porter stemmer. O algoritmo de lovins foi o pioneiro na área e influenciador de diversos algoritmos posteriores, sendo composto por 294 sufixos, 29 condições e 34 regras de transformção, dividido em 2 etapas. Na primeira etapa é feito uma busca pelo sufixo de maior tamanho na palavra e que satisfaça as condições do algoritmo, e dependo do caso, é removido. É importante notar que o comprimento mínimo do stem deve ser igual a 2, sendo, inclusive, estaa atividade realizada em sala, em que tínhamos acesso às regras de sufixos e às condições com o objetivo de reduzir palavras que eram pedidas. Tanto o algoritmo de lovins quanto o algoritmo de porter eliminam e removem os sufixos das palavras, sendo que para cadapalavra não é necessário conhecimento prévio para sua redução. Para o caso do algoritmo de porter, mais utilizado atualmente, é mais completo e simples do que o algoritmo de lovins. Para o português, há o algoritmo orengo, pioneiro, constituído por 199 regras distribuídas por 8 passos e considerando uma lista de exceções. Quando analisamos a eficácia do stemming, é importante ressaltar que nem sempre ele aumenta as chances de uma acurácia alta, podendo inclusive influenciar negativamente nos resultados, a depende da sua aplicação. Algumas considerações finais, não é eficaz o processo de stemming utilizando uma base de dados como dicionário, pois demanda um tempo computacional elevado; prefixos não são eliminados por não possuir motivo teórico em tal escolha; na literatura há diversas referências sobre o assunto; há dois termos que podem ocorrer devido ao stemming: overstemming, quando é removido uma parte do radical junto com o sufixo e understemming, quando o sufixo não é removido ou é apenas removido parcialmente.</td>
</tr>

<tr>
<td bgcolor=#99e699>Lucas Theodoro Guimaraes de Almeida</td>
<td> Sabemos que no português existe um problema da flexão das palavras, onde a mesma palavra pode ser apresentada de diversas forma, para tentar solucionar isso aprendemos uma técnicas Steamming e foi apresentado o conceito de Lemmatization.  Lemmatisation conceito  O Lemmatisation ele utiliza um dicionário como apoio para reduzir a palavra até a sua raiz, assim as suas reduções são melhores, porém o seu custo operacional é alto.  Steamming  A ideia desse algoritmo é fazer uma heurística na tentativa de conseguir a raiz da palavra sem precisar consultar um dicionário ( radicalização ), ou seja, precisamos que ele seja rápido, pois podemos analisar um texto muito grande de maneira rápida e temos indícios que a utilização de um dicionário externo quase nunca vai compensar pelo seu custo computacional, pois teremos uma resposta muito parecida sem ou com o dicionário. Infelizmente nem tudo é perfeito um Stemmer pode apresentar duas falhas um: Overstemming ( removeu mais trechos do que devia, assim perdemos informação do radical ) e Understemming ( quando removê mais trechos do que devia ). Para isso existem três algoritmos mais conhecidos que fazem essa tarefa: Lovins stemmer, Porter stemmer, sample text e Orengo, no caso os três primeiros foram idealizados em inglês, já o último em português. O algoritmo de Lovins é composto por 249 sufixos, 29 condições, 34 regras de transformação e duas etapas ( procurar o sufixo da palavra na lista de sufixos e verificar se esse está na coerente, após isso aplica a transformação ), o mesmo foi feito em 1969. O algoritmo de Porter veio em 1980, sendo mais eficiente na radicalização das palavras, assim tornou mais popular, o início do algoritmo é primeiramente uma busca em uma lista de sufixos, quando é encontrado ele passa os seguintes parâmetros Id da regra,  o sufixo, a substituição, o tamanho da palavra do sufixo e da substituição menos um e uma função de validação. Na stemmer da Orengo temos 199 regras distribuídas por 8 passos e uma lista de excepções, os passos são: A redução do plural para o singular, depois a troca da palavra do feminino para o masculino, as reduções argumentativa, adverbiais e substantivas, no próximo passo ele verifica se algum sufixo foi retirado, caso sim ele só retira os acentos, caso não ele aplica a redução verbais, nesse passo ele verifica novamente se algum sufixo foi retirado, caso sim ele retira os acentos, caso não ele aplica a redução de vogais e a remoção dos acentos.  </td>
</tr>

<tr>
<td bgcolor=#99e699>Lucas Zanferrari Caraca</td>
<td> A revisão da aula anterior consistiu em uma explicação do conceito de normalização de palavras e das técnicas mais importantes para esta tarefa. São elas: stemming e lemmatization. Stemming consiste em podar e substituir sufixos de palavras até que cheguemos as suas chamadas raízes. As regras para que estas podas e substituições ocorram variam de acordo com a língua do texto inicial e com o algoritmo de Stemming escolhido. A terminologia associada as técnicas foi apresentada. No caso de Stemming, "stem" é uma parte de uma palavra, e "stemmer" é o artefato, ou seja, algoritmo que utilizado para efetuar a normalização das palavras de um texto. Analogamente, para a Lemmatization, "lemma" é a forma básica de uma palavra (o algoritmo consiste em reduzi-las a "lemmas"), e o artefato é chamado "lemmatizer". Existem diferentes tipos de algoritmos de stemming. Dois dos mais populares são o algoritmo de Lovins e o de Porter. O mais antigo e pioneiro na área foi o de Lovins. Publicado em 1968 por Julie Beth Lovins, ele é composto por 294 sufixos, 29 condições, 34 regras de transformação e consiste em apenas 2 etapas, portanto é rápido. O algoritmo de Lovins consiste em procurar primeiro pelo sufixo de maior tamanho que satisfaz sua respectiva condição (vide tabela) em uma palavra, depois removê-lo e aplicar a regra de transformação associada aos últimos caracteres da palavra com o sufixo removido (caso haja alguma). Fazendo isso para todas as palavras no texto, todo o seu conteúdo será normalizado. O algoritmo de Porter foi publicado em 1980 por Martin Porter. Ambos são muito parecidos nos sentidos de que não requerem conhecimento prévio sobre o idioma do texto e trabalham removendo sufixos das palavras nele contidas, mas o de Porter é mais completo e simples. É o stemmer mais popular até a presente data. Ele possui validações de casos específicos em que o algoritmo de Lovins incorria na perda de informação útil das palavras. Um algoritmo de stemming para o português foi apresentado. Ele se chama Orengo, foi publicado em 2001 por Viviane Moreira Orengo e conta com 199 regras distribuídas por 8 passos. Uma tabela com algoritmos de stemming para outras línguas foi mostrada, mas sem entrar em detalhes. Por fim, começou-se a explicar com mais profundidade o processo de de lemmatization, que consiste na aplicação de técnicas para remover a flexão das palavras flexionadas do texto, de forma a reduzi-la a sua forma no dicionário disponível.</td>
</tr>

<tr>
<td bgcolor=#99e699>Higor Carmanini Barbosa</td>
<td> A aula retoma o tema da normalização das palavras, tema importante para o tratamento e interpretação geral de textos, uma vez que palavras de mesmo sentido podem tomar diferentes formas, como flexões de gênero, grau ou número. Buscadores online já se aproveitam de normalização para apresentar ao usuário resultados semelhantes de alta relevância. A radicalização de palavras se dá por duas técnicas mais importantes: stemming e lemmatization. O stemming consiste em reduzir a palavra ao seu radical morfológico, como gat para [gato, gata, gatos, gatas], através de remoções de afixos derivacionais que dependem de linguagem. Geralmente pode ser representado por conjuntos de regras, sempre atrelados à linguagem. Já o lemmatization consiste em reduzir à sua forma básica, mas ainda legível e semanticamente completa, como produzir para [produção, produziram, produtivo]. Para o stemming existem dois algoritmos principais: o de Lovins e o de Porter. Lovins, uma linguista computacional de Chicago, criou um stemmer  para o idioma inglês baseado na remoção de 294 sufixos condicionados por 29 diretivas e 34 regras de transformação, divididos em duas etapas simples, num algoritmo linear. Sua execução consiste em remover o maior sufixo que satisfaça sua condição restritiva e, então, aplicar a regra de maior nível hierárquico que satisfaça os sufixos definidos. Isso torna o algoritmo simples, pois cada palavra pode ser analisada individualmente. Porter foi um bibliotecário que criou um stemmer inicialmente projetado para recuperar as informações da biblioteca onde trabalhava. É um algoritmo mais completo e mais simples do que o de Lovins, sendo o mais utilizado atualmente, quase 40 anos depois de sua criação, em 1980. Contém 5 passos, com regras e operações de validações. Existem, também, algoritmos de stemming para o português, como o Orengo, criado por Viviane Moreira em 2001 como projeto de doutorado na Middlesex University. É constituído de 199 regras distribuídas por 8 passos, considerando uma lista de exceções. O algoritmo de Porter já foi modificado para atender vários idiomas, como o próprio português, por ser eficiente. Entretanto, existem algumas considerações sobre os stemmers: - Utilização de um dicionário aumentaria eficácia, mas reduziria eficiência. Vale a pena? Não - Prefixos: porque não são considerados? - Muita perda de informação? - Overstemming e Understemming: não alcançar os objetivos ou ir além deles - Nomes próprios O lemmatization geralmente usa dicionários de palavras, com uma heurística mais sofisticada.</td>
</tr>

<tr>
<td bgcolor=#99e699>Gustavo Tino Ferreira</td>
<td> A normalização das palavras é muito importante em diversos contextos. Por exemplo, os motores de buscas (google, bing) não apresentam apenas resultados que contenham a palavra exata digitada. O sistema retorna palavras parecidas, com um radical semelhante. O processo de stemming é baseado em uma heurística que corta as extremidades das palavras afim de reduzí-las à algo próximo de seu radical. Desta forma, palavras que tem o mesmo significado e são escritas ligeiramente diferentes, são agregadas e tomadas como a mesma (amig representa tanto amigos quanto amigão). Isso é feito apartir de um conjunto de regras e dicionários de sufixos, portanto, um algoritmo para lingla inglesa não funcionaria nas outras linguagens. O algoritmo de Lovins foi o pioneiro da língua inglesa. Apesar de não ser muito utilizado hoje em dia, é de grande importância para a computação, pois a maioria dos algoritmos de stemming foram baseados neste algoritmo. O algoritmo segue uma tendência gulosa e consegue rodar em tempo linear. Dado uma palavra e seu maior sufixo possível (aquele que ignora apenas as duas letras iniciais, pois poderiam ocorrer um stemming onde a palavra vira uma letra só, o que não faz sentido), o programa procura numa tabela de sufixos (acompanhados de uma condição) com o mesmo tamanho. Se o sufixo da palavra estiver na tabela de sufixos e supre a condição necessária, o sufixo atual é cortado da palavra. Caso o sufixo não esteja na tabela, seu tamanho é reduzido e o processo se repete. Isso ocorre até um sucesso, ou então a palavra não pode ser reduzida mais. Após a remoção de um sufixo, a palavra passa por um tratamento, que retira letras dobradas do final e uma série de outras correções. O programa mais famoso para a língua inglesa é o algoritmo de Porter. Que funciona de maneira similar ao de Lovins, porém mais simples e completo que seu antecessor. O primeiro algoritmo de stemming para a língua portuguesa a ser amplamente divulgado foi o algoritmo de Orengo, funcciona de maneira parecida com o de Lovins, porém faz o uso de uma lista de exceções. Uma outra forma de reduzir palavras poderia ser com base em um banco de dados contendo todas as palavras e seus radicais, porém isso pode não oferecer uma vantagem tão grande e, do ponto de vista computacional, custa muito mais caro.</td>
</tr>

<tr>
<td bgcolor=#99e699>Andre Oliveira Macedo</td>
<td> Assim como mencionado na aula anterior, a normalização de palavras pode ser utilizada para facilitar a busca por documentos, sendo que as técnicas mais relevantes são stemming e lemmatization. O primeiro possui o objetivo de obter a raiz da palavra, sendo um processo heurístico de cortar as extremidades da palavra e remover afixos derivacionais. Já o segundo é obtém os lemmas, o significado básico da palavra.  Como cada língua possui diferentes características, geralmente o desempenho de cada método varia, sendo mais ou menos eficaz dependendo da situação. Vale notar que a saída dos stemmers não é necessariamente uma palavra raiz e eles podem cometer erros também, porém podem ser úteis para generalizar e costumam ser eficientes. Algoritmo de Lovins: Um dos primeiros algoritmos de muitos steemers do inglês, possui apenas duas etapas, onde busca pelo maior sufixo na palavra e em seguida aplica as regras para transformar a palavra, removendo o final. Basicamente realiza uma busca em uma tabela de 294 finais, sendo 29 condições e 35 regras de transformação. Remove o sufixo mais longo e grava a nova palavra em uma tabela diferente, que pode buscar por uma palavra raiz. Algoritmo de Martin F. Porter: Voltado para o inglês, possui cinco passos, remove sufixos e atualmente é o stemmer mais utilizado sendo mais completo que o anterior.  Possui a ideia base de que a maior parte dos sufixos é composto de uma combinação de sufixos menores e mais simples. As regras são aplicadas até que uma delas passe nas condições, se uma regra é aceita o sufixo é removido. Porém, é mais lento que o algoritmo de Lovins. Algoritmo de Orengo: Um dos primeiros algoritmos para o português, foi publicado em 2001 e possui oito passos, onde ocorre algumas reduções, como: plural, feminino, tamanho, advérbio, pronome, verbo e remove acentos, sufixos e se necessário, vogais. NLTK: O python possui um toolkit de linguagem natural chamado NLTK e possui ferramentas de stemming e lemmatization, além de bibliotecas para classificação, conversão, análise semântica e outros. Há um demo disponível com alguns algoritmos mencionados anteriormente em:http://text-processing.com/demo/ stem/ Também é possível remover os prefixos das palavras, mas pode ocorrer uma remoção maior do que a desejada, ou remover menos do que se deveria. Essa é uma forma de medir o desempenho do algoritmo.</td>
</tr>

<tr>
<td bgcolor=#99e699>Juliane Kristine de Lima</td>
<td> A normalização de texto é a busca de reduzir várias palavras com o mesmo significado semântico para um único tipo de palavra. Na língua portuguesa, por exemplo, as palavras normalmente diferem em tempos verbais, gênero e número, sendo que no processo de normalização, todo o grupo de palavras é transformado em um único elemento. Muitas vezes, para esse processo, realiza-se redução, simplificação ou radicalização das palavras, como na técnica de stemming. Nesta técnica, busca-se reduzir as palavras à sua raiz comum (gat representaria gato, gata, gatos, etc.). De modo geral, stemming é uma técnica de heurística, em que as extremidades das palavras são removidas, e mais algumas regras são implementadas dependendo da língua do texto. A técnica difere de lemmatisation, pois nesta técnica as palavras são reduzidas à sua forma básica, sendo necessário um dicionário de palavras como base para o algoritmo. O primeiro algoritmo de stemming foi criado em língua inglesa por Julie Lovings, em 1968. O algoritmo é simples e possui apenas duas etapas, que são realizadas no texto a partir de uma base de dados. Não é necessário um dicionário de palavras, a redução pode ser realizada somente com as técnicas estruturadas. Outro algoritmo importante para a língua inglesa é o de Martin Porter, que também é o mais utilizado atualmente. Este é considerado mais simples que o de Lovings, mas ambos têm como principal carcaterística cortar o maior sufixo possível pré-determinado. Na língua portuguesa, o primeiro algoritmo de radicalização publicado é o de Viviane Orengo, mas posteriormente foram criados outros que utilizam regras diferentes. Porém, a ideia principal dos stemmers é comum em todos os casos. Algumas considerações foram realizadas a respeito do stemming. O uso de dicionários não é realizado, pois requer uma capacidade computacional desnecessária considerando que o resultado pode ser atingido com algoritmos mais simples. A eliminação de prefixos não é realizada normalmente, apesar de não haver justificativas teóricas para evitá-la. O interesse na prática do stemming é bastante recente na literatura, especialmente após a criação dos primeiros algoritmos. Além disso, algoritmos de stemming trazem o risco de perda de informação, seja por overstemming (quando parte do radical é removida em conjunto com o sufixo)  ou understemming (quando o sufixo é removido de forma incompleta).  </td>
</tr>

<tr>
<td bgcolor=#99e699>Jair Pereira Junior</td>
<td> Aula 4. Normalização de texto. A motivação para normalizar textos é poder reconhecer uma mesma palavra escrita de forma diferente, como quando com letras maiúsculas. Ou palavras de mesmo valor semântico mas flexionadas em gênero, número e grau, por exemplo. Existem diferentes técnicas para normalização de texto, mas as mais conhecidas são Stemming e Lemmatization. Stemming é uma heurística dependente da linguagem que reduz a palavra à sua raiz.  A raiz em questão não é a definição linguística, mas sim uma parte da palavra que carrega o seu significado. Existem diferentes algoritmos de stemming, mas os mais conhecidos são Lovins stemmer e Porter stemmer. Ambos funcionam de forma similar, apenas com pequenas diferenças quanto as suas regras. O algoritmo de Lovins é composto por duas etapas. A primeira localiza e remove o maior sufixo entre os 294 possíveis, se existir, e aplica uma das 29 condições relacionada ao sufixo removido. A segunda tenta aplicar uma das 34 regras de transformação, se possível. O algoritmo de Porter é mais completo e mais simples do que o de Lovins, sendo o stemmer mais utilizado atualmente. Possui 5 passos compostos de aplicar uma regra e fazer uma operação de validação. O algoritmo de Porter e o algoritmo de Lovins foram criados originalmente para o inglês. Orengo é um stemmer para o português, criado em 2001. Possui 8 passos com 199 regras mais uma lista de exceções. O algoritmo de Porter foi amplamente adaptado para várias outras línguas como português, russo, espanhol, italiano etc. O website &lt;http://text-processing.com/demo/stem/&gt; pode ser utilizado para verificar o processo de stemming e outros algoritmos de normalização. Considerações sobre stemmers. Poderia-se utilizar um dicionário para comparar as palavras reduzidas, mas necessita de maior tempo computacional, fazendo o esforço não valer a pena. Stemmers, em geral, removem o final de palavra, mas poderiam ser adaptados para removerem prefixos também. São métodos heurísticos, podem causar perda de informação para algumas palavras Pode acontecer overstemming, quando uma parte importante do radical é removida, e understemming, quando o sufixo é removido parcialmente.</td>
</tr>

<tr>
<td bgcolor=#99e699>Marcos Freitas Parra</td>
<td> Um algoritmo de stemming é um procedimento computacional que reduz todas as palavras com a mesma raiz ou a mesma baste, para uma forma comum, geralmente removendo cada palavra de seus sufixos derivativos e inflexão. Pesquisadores em muitas áreas de linguística computacional e recuperação de informações acham isso um passo desejável, mas por razões variadas. Na análise morfológica automatizada, a raiz de uma palavra pode ser de interesse menos imediato do que seus sufixos, que podem ser utilizados como pistas para a estrutura gramatical. No outro extremo, sufixos são encontrados podem ser subsidiários do problema de removê-los consistentemente o suficiente para obter conjuntos de bastes exatamente correspondentes. Embora a forma do algoritmo varie de acordo com sua aplicação, certos problemas linguísticos são comuns a qualquer procedimento derivado. O Lovins stemmer possui 294 terminações, 29 condições e 35 regras de transformação. Cada final está associado a uma das condições. No primeiro passo, é encontrado o final mais longo que satisfaz suas condições associadas e é removido. Na segunda etapa, as 35 regras são aplicadas para transformar o final. O segundo passo é feito se um final é removido ou não no primeiro passo. As regras no algoritmo de Porter são separadas em cinco fases distintas numeradas de 1 a 5. São aplicadas às palavras no texto a partir da fase 1 e passando para a fase 5. Além disso, elas são aplicadas sequencialmente uma após a outra como comandos em um programa. Assim, no imediatamente seguinte, especificamos o algoritmo Porter em uma linguagem pseudo-programação cujos comandos assumem a forma de regras para a substituição do sufixo. O algoritmo de Orengo está baseado em regras para a remoção de sufixos. No total, apresenta 199 delas. Contudo, as regras apresentam exceções, permitindo uma redução em termos de overstemming (quando parte do radical é removida pelo algoritmo) de 5% (Orengo 2004: 49 e 51). Pode-se dizer, então, que o algoritmo é um misto das duas alternativas apresentadas por Russell e Norvig.</td>
</tr>

<tr>
<td bgcolor=#99e699>Marcos Seiti Suzuki</td>
<td> No português por exemplo, as palavras podem ter flexão em número, grua ou gênero, conjugações verbais entre outras variações. A normalização das palavras é a redução ou radicalização da palavra. Existem duas técnicas de normalização: Stemming - Stem é parte de uma palavra, geralmente são heurísiticas que eliminam as extremidades da palavra, geralmente os sufixos, regras. Não é considera a classe gramatical da palavra reduzida. Lemmanization - ação de reduzir a palavra em um lemma forma básica. Existem muitos algoritmos de Stemming, geralmente são desenvolvidos por linguistas, principalmente para a língua inglesa. Um desses algoritmos é o Algoritmo de Lovins, pioneira no ramo. As heurísiticas são compostos por 294 sufixos, 29 condições e 34 regras de transformação. O processamento utiliza apenas 2 etapas. A primeira etapa consiste em remover o maio sufixo possível com o radical tendo ao menos dois caracteres, esteja na lista pré-estabelecida e cumpra a condição associada a ela. A segunda etapa consiste em aplicar uma transformação no radical resultante da etapa 1, desde que se enquadre em uma das regras pré-estabelecidas. O algoritmo mais usado e famoso é o Algoritmo de Porter. É considerado um algoritmo mais completo e mais “simples” que da Julie Lovins. Utiliza cinco etapas, similar ao da Julie Lovins a etapa inicial consistem em remover o maior sufixo desde que o radical possua 3 caracteres. O primeiro algoritmo de radicalização para o português, amplamente divulgado, foi desenvolvido por Viviane Moreira Orengo. Utiliza 199 regras distribuídas em 8 passos. A não remoção de prefixos não possui nenhum embasamento teórico apesar de não ser feito. Caso a heurística não seja bem elaborada é comum ocorrer perda de informações ou não ser otimizada ao realizar a radicalização. Existem duas métricas para medir esse desempenho: Overstemming quando parte do radical é removido; Understemming quando o sufixo não é removido ou removido parcialmente. Lemmatisation é o processo de deflexão das palavras, sua heurística costuma ser mais sofisticada.</td>
</tr>

<tr>
<td bgcolor=#99e699>Wedeueis Braz da Silva</td>
<td> Normalização de texto: Steeming Um sistema de busca deve permitir que documentos indexados com diferentes nomes sejam recuperados usando quaisquer das suas formas de escrita. A “normalização de palavras” pode ser entendida como a redução ou a simplificação ou a radicalização de palavras, o que pode facilitar essa busca. Existem duas técnicas principais: Steeming e Lemmatization.  Steeming O processo de steeming consiste na redução das palavras à sua raiz, sem levar em conta a classe gramatical. Ex: amig: amigo, amigos, amiga, amigas, amigão, amiguinho… Geralmente refere-se a um processo de heurística que corta as extremidades das palavras, inclui frequentemente a remoção de afixos derivacionais e pode ser representado por um conjunto de regras que depende da linguagem. O resultado não necessariamente é a raiz na forma linguística e pode não ter significado.  Algoritmos de Steeming Steeming pode ser entendido como a redução das palavras em stems. Stem ? parte de uma palavra. Stemmer ? O artefato (programa)  Existem diferentes algoritmos, principalmente para o inglês.  Algoritmo de Lovins Algoritmo pioneiro influenciador de muitos stemmers. Desenvolvido por Julie Beth Lovins em 1968 é composto por 294 sufixos, 29 condições e 34 regras. O processamento é rápido, feito em apenas duas etapas: I – Procurar pelo sufixo de maior tamanho na palavra e que satisfaz as condições, então remover. II – As regras são aplicadas para transformar o final se um sufixo é removido na primeira etapa.  Algoritmo de Porter Algoritmo mais completo e mais simples que o de Lovins. Desenvolvido por Martin F. Porter em 1980, é o algoritmo de stemming mais usado atualmente. Tem 5 passos bem definidos usando regras de operação e validação. É similar ao de Lovins, cada palavra é sequencialmente comparada com o maior sufixo possível, se casou o sufixo então removê-lo da palavra.  Algoritmo de Orengo Publicado por Viviane Moreira Orengo em 2001 foi a primeira versão de um algoritmo amplamente divulgado para a língua portuguesa. É constituido por 199 regras distribuídas por 8 passos e considera uma lista de exceções.</td>
</tr>

<tr>
<td bgcolor=#99e699>Ana da Silva de Paula</td>
<td> Num sistema de recuperação de informação, é necessário que se encontre documentos e páginas que tenham os termos pesquisados em qualquer uma de suas formas; por exemplo, se procurarmos por /amar/, devem ser encontrados documentos com /amor/, /amado/ e /amante/, por exemplo. É possível transformar as palavras flexionadas em uma forma geral, e esse processo é chamado de normalização. Em português, as palavras podem ser flexionadas nos tempos verbais, em gênero, número e grau. A normalização de palavras consiste na simplificação ou radicalização da palavra. Esses processos são elaborados através de duas técnicas: o stemming e a lemmatization. O processo de stemming consiste na redução da palavra para sua raiz, sem considerar a classe gramatical a qual ela pertence. Em geral, ela consiste em cortar a palavra de modo a retirar os sufixos, mantendo somente a parte da palavra que não varia nas derivações. A lemmatization, por outro lado, visa retornar a palavra derivda à sua forma no dicionário -- em portugues, na forma infinitiva, para verbos, e na forma singula e masculina nas demais classes. Há muitos algoritmos de stemming, principalmente na íngua inglesa. Eles consistem, de maneira geral, de um conjunto de sufixos, de condições de remoção dos sufixos e de regras de transformação, sem que seja necessário nenhum tipo de conhecimento prévio ou consult.  Os algoritmos de normalização são uma etapa anterior ao processamento, portanto, devem ser velozes e não consumir muitos recursos computacionais; o stemming é muito eficiente nesse aspecto. O uso de um dicionário para compara os resultados é uma opção para aumentar a acurácia, mas estudos indicam que os ganhos não valem os recursos investidos. O stemming é passível de erro, já que nem todas as palavras seguem as mesas regras. Podem ocorrer erros por excesso, quando são removidos mais que o sufixo, ou por falta, quando o sufixo não é removido totalmente. Além disso, a remoção do sufixo pode acaretar na perda excessiva de informação.</td>
</tr>

<tr>
<td bgcolor=#99e699>Eduardo da Silva Cruz</td>
<td> Normalização de palavras é um sistema de busca deve permitir que documentos indexados com diferentes nomes sejam recuperados usando quaisquer das suas formas de escrita. Em textos da língua portuguesa temos diferentes palavras flexionadas em gênero, número ou grau, além de inúmeros tempos verbais distintos. E. g. Trabalhador - Trabalhadora, Degrau - Degraus, Amigo -Amigão. O conceito de normalização das palavras foi iniciado com a introdução de duas técnicas importantes : Stemming: O processo de stemming consiste em reduzir a palavra à sua raiz (sem levar em conta a classe gramatical). Stemming geralmente refere-se a um processo de heurística que corta as extremidades das palavras inclui frequentemente a remoção de afixos derivacionais.  amig : amigo, amiga, amigão gat : gato, gata, gatos, gatas Para ambas as técnicas existem diferentes algoritmos, principalmente para o inglês. O algoritmo de Lovins é composto por 294 sufixos, 29 condições e 34 regras de transformação. O seu processamento é rápido com apenas duas etapas O algoritmo Orengo de Stemming para Português foi desenvolvido por Orengo, Viviane Moreira, & Huyck, Christian. (2001). A Primeira versão amplamente divulgada de um algoritmo de radicalização para a língua portuguesa: Constituído por 199 regras distribuídas por 8 passos e considera uma lista de excepções Em contra ponto com essas técnicas apresentadas, poderia ser utilizado uma se dados (dicionário) para comparar as palavras reduzidas. Porém esse procedimento requererá maior tempo de processamento computacional e mesmo consumindo maior tempo, o esforço investido poderia não valer a pena. Também é possível eliminar os prefixos em vez dos sufixos, não existe nenhum motivo teórico para não considerar a eliminação de prefixos nos stemmers O algoritmo de stemming não deveria permitir a perda de muita informação, contudo, se as tecnica não for bem aplicada isso pode acontecer. É importante utilizar medidas de desempenho para verificar o que está ocorrendo com a informação e detectar casos de Overstemming ou Understemming</td>
</tr>

<tr>
<td bgcolor=#99e699>Hiago Lucas Cardeal de Melo Silva</td>
<td> Stemmming - Nas linguagens naturais em geral, as palavras possuem diversas flexões, ou seja, cada palavra pode ser apresentada de diferentes formas. O processo de normalização consiste na simplificação/redução/radicalização destas palavras. Um dos processos de normalização é chamado stemming. O stemming consiste em remover as extremidades das palavras para converte-las para sua forma simplificada/radicalizada. Um dos algoritmos pioneiros de stemmimng é o algoritmo de Lovins. Seu processo é simples e consiste em apenas duas etapas. 1) Procurar pelo sufixo de maior tamanho na palavra e que satisfaz as condições -&gt; remover 2) Aplicar as regras de transformação no final da palavra. Estas regras devem ser aplicadas independente de ter havido remoção ou não. Existe outro algoritmo popular para stemming da lingua portuguesa. Este é chamado de algoritmo de Porter. É mais completo e mais simples que o algoritmo de Lovins e portanto, é o mais usado atualmente. Existem algoritmos de stemming para o portugues também. É o caso do algoritmo ORengo. Consiste em 199 regras distribuídas em 8 passos, além disso, é capaz de considerar exceções. Existem algumas considerações finais a serem ditas a respeito do processo de setemming !) Uso de um banco de dados: seria possível fazer o processo de stemming através de um banco de dados. Ou seja, um banco onde cada flexão da palavra faria referência ao radical. No entanto, esse procedimento iria requerer muito processamento e, talvez, não valesse a pena. 2) Por que não eliminar prefixos? Não há motivos teóricos para desconsiderar a eliminação de prefixos. 3)O algoritmo de stemming nao deve permitir a perda de muita informação 4)É possível medir o desempenho de um processo de stemming através de dois parâmentros. Oversteming: quando é removido não só o sufixo, mas também uma parte do radical. Understemming: quando o sufixo não é removido completamente.</td>
</tr>

<tr>
<td bgcolor=#99e699>Thais Larissa Batista de Andrade</td>
<td> A quarta aula da disciplina focou em um dos últimos tópicos abordados na aula anterior, a técnica de normalização de textos chamada Stemming. Um stem é parte de uma palavra e stemming é a ação de reduzir essa palavra. Utilizamos essa técnica para encontrar palavras parecidas e isso é possível através da radicalização delas. Nessa método, são cortados os sufixos, elementos que adicionados à um radical formam uma nova palavra, a língua portuguesa possui três tipos de sufixos: nominal, verbal e adverbial. Alguns exemplos de radical e suas palavras derivadas são: folha - florescer, florear; amor - amar, amável. O algoritmo deve ser capaz de identificar quais são os sufixos da palavra e reduzi-la à ela. O algoritmo pioneiro para a língua inglesa foi o algoritmo de Lovins, que foi uma linguista computacional, formada na Universidade Brown. O Algoritmo de Lovins possui apenas 2 etapas, ele primeiro busca o sufixo de maior tamanho e que satisfaça uma das 29 condições criadas e o remove caso sim. Depois disso, se o sufixo é removido ou não, uma das 34 regras de transformação são aplicadas para mudar o final. Apesar de possuir menos passos, o stemmer mais utilizado para a língua inglesa atualmente é o do Martin Porter, por ser mais simples mesmo possuindo mais passos. Em 2001, foi divulgado um algoritmo de radicalização para a língua portuguesa. Ele está baseado em 199 regras que têm como intuito a remoção de sufixos através de 8 passos. Foi desenvolvido por Viviane Moreira Orengo e por isso leva seu nome, sendo conhecido como Orengo. Esse tipo de algoritmo é considerado uma heurística e portanto sua implementação e otimização dependem das regras adotadas para cada língua.</td>
</tr>

<tr>
<td bgcolor=#99e699>Maira Zabuscha de Lima</td>
<td> Para buscar um conjunto de palavras que representam um mesmo termo, exemplo: controlador, controladores, controle; digital, digitais; é necessário normalizar as palavras executando a radicalização, ou seja, eliminar flexão em gênero, número ou grau, e conjugação. A técnica do stemming reduz a palavra à sua raiz, efetuando uma poda no afixo da palavra, e depende da linguagem. A palavra resultante (stem) pode não ser em si uma palavra com significado. A técnica do lemmatization reduz as palavras para uma forma básica mas que em si é uma palavra com significado. Existem vários algoritmos que efetuam stemming, principalmente para o idioma inglês. O algoritmo de Julie Lovins é um algoritmo pioneiro. Nele são definidos 294 sufixos, 29 condições e 34 regras de transformação que são aplicadas em 2 etapas. O algoritmo de Martin Porter também elimina sufixos, porém é mais completo e mais simples, e é o mais utilizado atualmente. O algoritmo de Viviane Orengo foi o primeiro algoritmo de stemming para a língua portuguesa. Possui 199 regras efetuadas em 8 passos. A linguagem Python possui a biblioteca NLTK para stemming e lemmatization. Uma forma de melhorar o resultado dos algoritmos seria comparar o texto produzido com um dicionário do idioma, entretanto isto requereria mais tempo de processamento. Usar o dicionário para corrigir as palavras podadas seria o lemmatization. Os algoritmos apresentados eliminam sufixos mas poderiam (ou deveriam) eliminar também prefixos. Podem ocorrer o overstemming ou understemming, portanto os algoritmos de stemming, buscando evitar a perda de informação, evitam principalmente o overstemming com as regras que determinam o tamanho mínimo do stem (palavra podada) e também exigindo que permaneça pelo menos uma vogal.</td>
</tr>

<tr>
<td bgcolor=#99e699>Leandro Akira Tochiro</td>
<td> Nessa aula, vamos nos familiarizar melhor com os conceitos de "stemming" e conhecer alguns algoritmos frequentemente usados nessa área, tais como "Loving" e "Porter". Da aula passada, lembramos que stemming é uma técnica de normatização das palavras, que consiste em reduzir e/ou simplificar as palavras e transformá-las em seu radical ou raíz. Por exemplo, "casarão, casebre, casinha, casamento, casa" podem ser reduzidos para a raiz "cas". Outro exemplo: o grupo de palavras "reflexo, refletido, refletir, reflexão" podem ser substituídos por '"refle". No idioma inglês, temos alguns algoritmos famosos, como lovins e porter. O primeiro consiste em cortarmos os possíveis sufixos de uma palavra de acordo com o algoritmo, sempre respeitando as regras para cada corte. Essa etapa é feita do maior sufixo para o menor (quantidade de letras), em que a palavra resultante deve ter no mínimo duas letras. Para cada sufixo, o algoritmo apresenta regras condicionais para a retirada, e caso a palavra não cumpra tal condição, é preciso tentar retirar uma quantidade de letras imediatamente menor. Nesse caso, são 29 condições.  Exemplo: National -&gt; Nat (ional). Nesse caso, a palavra raíz é "nat". No segundo (Porter), substituimos a terminação da palavra resultante da primeira etapa de acordo com uma tabela do algaritmo. Exemplo: matrix -&gt; matric. Nesse caso, são 34 regras. É importante ressaltar que esses algaritmos foram feitos para o idioma inglês e portanto, para a lingua portuguesa seria preciso criar nosso próprio algoritmo, visto que na nossa lingua temos ainda mais variações de grau, gênero, número, família etc.</td>
</tr>

<tr>
<td bgcolor=#ffd480>Mauro Gil Dias Leite</td>
<td> Em textos da língua portuguesa temos diferentes palavras flexionadas em gênero, número ou grau, além de inúmeros tempos verbais distintos. A “normalização de palavras” pode ser entendida como a redução, a simplificação ou a radicalização de palavras. Para isso temos duas técnicas importantes: Stemming e Lemmatization. O processo de stemming consiste em reduzir a palavra à sua raiz (sem levar em conta a classe gramatical),  não é a raiz da palavra e a palavra pode ficar sem significado. Stemming (a ação de reduzir em stems)                         X Lemmatization (a ação de reduzir em Lemmas) Stem: Parte de uma palavra                                            X Lemma: Forma básica da palavra Stemmer: O artefato (programa)                                     X Lemmatizer: O artefato (programa) Existem diferentes algoritmos como por exemplo: Lovins, Porter, Orengo. O algorítimo de Lovins foi o primeiro e é composto por 294 sufixos, 29 condições e 34 regras de transformação com um processamento de apenas 2 etapas. A primeira é procurar pelo sufixo de maior tamanho e removê-lo de acordo com as condições de cada sufixo, a segunda é aplicar a regra de transformação de final correspondente com o que sobrou da palavra. O algorítimo de Porters é um pouco mais longo, mas é mais simples e é o stemmer mais utilizado atualmente. Todos os 5 passos foram bem definidos: Usa-se regras e operação de validação. Cada palavra é comparada sequencialmente com o maior sufixo possível e então removê- lo.</td>
</tr></table><br> <hr> Arquivo gerado por um programa.<br><p></body></html>