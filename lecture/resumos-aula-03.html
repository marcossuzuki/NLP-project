<html><head><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"></head><body><h3>Resumos de aula: PLN - 2018</h3><br> <i>Nota: Arquivo gerado por um programa.</i><br><p><table border=1> <tr> <td>Nome</td> <td>Resumo</td> </tr>

<tr>
<td bgcolor=#33cc33>Bruno Aristimunha Pinto</td>
<td> Iniciamos essa aula resgatando os conceitos da classe passada. Do teste de avaliação em sala, em específico na sexta questão foi nos proposto elaborar uma expressão regular que segmente duas palavras repetidas de forma seguida. Para tanto, observamos a necessidade de um operador de memória que registre e aplique o valor encontrado anteriormente. Temos que esse armazenamento consiste em delimitar a expressão com os parênteses () e resgatar o valor com o comando \i (dependendo do interpretador pode ser convencionado como $i), sendo i um índice numérico pertencente aos números naturais maiores que 0, e.g., {1, 2, 3, 4}. Indo além, podemos misturar essa memória da expressão regular nos parênteses de forma “recursiva”; para tanto, temos somente que nos atentar que ao contrário da prioridade estabelecida na matemática, aqui indicamos o primeiro grupo encontrado, ou seja, o grupo mais externo como primeiro. O índice correspondente aos demais segue a numeração da esquerda para direita. De forma didática, suponha uma lista de e-mail que queremos realizar uma busca com a expressão ((\w+)\.(\w+))@\”i”, se estamos buscando o e-mail jesus.mena@jesus.mena, logo i deve ser igual à 1, se estamos buscando jesus.mena@jesus (i = 2); ou se estamos buscando jesus.mena@mena, i deverá ser 3. Seguindo, convencionamos um intervalo na tabela ASCII que devemos buscar e considerar para compor uma palavra. Isso ocorre pois ao contrário do inglês temos acentos que compõem nossa linguagem. Analisando a tabela estendida chegamos à expressão regular que melhor segmenta as palavras em um texto corrido para nosso idioma (“[-'a-zA-ZÀ-ÖØ-öø-ÿ0-9]+”). Com essa ferramenta chegamos, em uma das etapas fundamentais do processamento de linguagem natural, na extração de palavras (tokenização). Definimos a quantidade de palavras no texto corrido com tokens, e também estabelecendo um vocabulário, um conjunto de type, V, que de forma grosseira simboliza quantas palavras diferentes há no texto. Essas meta-informações acerca do texto estabelecem métricas que possibilitam extrair padrões relativos aos textos. Durante muito tempo tentou-se estabelecer uma relação entre a variedade de palavras e a quantidade de palavras, uma heurística conhecida pertence à Lei de Herdan (1960) e Lei de Heaps (1978): |V|=kN^?, sendo N o número de tokens, k uma constantes positiva, ? entre [0, 1] e dependendo do tipo e do gênero do texto. Um caso de estudo conhecido envolve o manuscrito de Voynich, um texto que possui sistema de escrita não identificada e uma linguagem ininteligível. Esse texto, apesar de não ter uma possível interpretação, possui um forte indicativo de que há uma lógica por trás da linguagem pelos atributos encontrados V e N (LANDINI, G., 2009). Com as palavras em mãos, chegamos na normalização das palavras, o processo de normalização levanta uma pergunta, o que é realmente uma palavra? Geralmente, na própria língua não uma há uma definição generalista capaz definir o que constitui uma palavra. Podemos considerar a pontuação presente na escrita como uma palavra? Devemos contar como palavras distintas quando uma palavra apresenta em CAIXA ALTA e outra em caixa baixa (ATENÇÃO ou atenção)? Há sentido contabilizar conjugações diferentes (estou e estamos)? Essa pergunta complicada tem que levar em conta o contexto da análise, se estamos processamento sentimentos há uma diferença quando uma palavra aparece em maiúscula e minúscula. Aqui, passamos a entender a necessidade de um claro objetivo em mente antes da análise, pois é esse objeto que norteará as escolhas metodológicas necessárias durante o processamento da linguagem natural. Aqui também, começamos a realizar considerações sobre a seleção do conjunto de palavras, o como devemos traçar recortes para verificar o quão bom os algoritmos estão sendo otimizados para critérios específicos. Na literatura temos conjuntos especializados para isso denotados como corpus.  O corpus são um conjunto de texto rotulados que possibilitam a validação e teste dos resultados obtidos. Convencionando o que buscamos como palavra, entramos em Lemmatization, isto é o processo de extração de lemas. Definimos como lema o conjunto lexical tendo palavras que representam a mesma ideia. Por exemplo, gatos e gato possuem o mesmo lema: gato. A grosso modo, através do lema temos uma simplificação de palavras que possuem formatos distintos. Essa simplificação à uma palavra raiz possibilita remover repetições e inflexões relacionadas à gramática. Para entendermos melhor, devemos considerar a morfologia como a pequena unidade significativa que compõe as palavras, nesse campo há uma ramificação das palavras considerando sua variação, e.g., inflexões, conjunções, entre outros. Comumente, a simplificação à um core possui um custo computacional alto, sendo necessários em contexto aplicados à tradução. Com as definições morfológicas e validação empírica, também observamos que comumente há palavras que não possuem um significado semântico importante. Em outras palavras, não contribuem para a macro ou micro análise desejada. Novamente, devemos nos atentar à aplicação. Em aplicações em que não buscamos extração somente por repetição e derivados, palavras como: a, um, para, que podem ser excluídas através de um stopword (também conhecida como stoplist). Stopword consiste de uma lista de palavras que são excluídas na tabulação dos dados. Devemos no atentar também sobre a extensão dessa lista, pois, se longa a mesma pode tirar informações importantes e necessárias na análise de dados. Discutimos em sala, o artigo de Dolimic, L. e Savoy, J. (2009) que concluíram que o número de palavras em um stopword deve ser próximo de 9 para uma melhor análise. Essa heurística traça um norte com relação à esse tipo de busca, no entanto, nos relembra da limitação em relação à seu uso. Podemos analisar que o uso de uma única stopword em diferentes aplicações pode gerar distorções em nossos dados. Assim como o uso de um único parâmetro de um algoritmos otimizado à uma base de dados específica. Se esse parâmetro for usado de forma irrestrita podemos reduzir significativamente métricas como acurácia e precisão. Encerramos o assunto sobre stopwords que em aplicações com maior rigor técnico devem construir as stopword manualmente. Finalizando a aula, discutimos algumas considerações não triviais. Até o momento, levamos em conta algumas premissas importantes para realizar todo o processo descrito, como por exemplo, a separação clara na língua de um espaço entre as palavras. Isso não é possível se generalizar em múltiplos idiomas, em específico em línguas aglutinantes e/ou línguas do tronco linguístico sino- tibetano (chinês, japonês, coreano, entre outros). Um algoritmo muito conhecido e consagrado para word tokenization em chinês é o Maximum Matching (também conhecido como algoritmo de Greedy) consiste de uma busca gulosa aplicada na separação de palavras. Esse algoritmo funciona da seguinte maneira: 1º passo, inicializamos o algoritmo com uma lista de palavras em chinês e uma string (frase a ser separada); 2º passo, começamos marcando um ponteiro para o começo da string; 3º passo, buscamos a maior palavra no dicionário que casa com o começo da string; 4º passo, movemos o ponteiro ao final do casamento de palavras; 5º passo, repetimos o passo 3. Essa técnica, infelizmente não possui bons resultados em inglês ou idiomas com construção semelhante. Esse método, apesar de ser um baseline conhecido, possui sucessores que modelam de forma probabilística essa separação. Finalizamos a aula dialogando brevemente um outro processo de normalização, que não busca uma palavra raiz e sim um recorte da palavra, conhecido como Stemming. Houve também, durante a aula, uma exposição prática na linguagem de programação Python, com inúmeros comandos e boas práticas para extração de palavras, com e sem stopwords.  Referências usadas: DOLAMIC, Ljiljana; SAVOY, Jacques. When stopword lists make the difference. Journal of the Association for Information Science and Technology, v. 61, n. 1, p. 200-203, 2010. HEAPS, Harold Stanley. Information retrieval, computational and theoretical aspects. Academic Press, 1978. HERDAN, Gustav. Type-token mathematics. Mouton, 1960. LANDINI, Gabriel. Evidence of linguistic structure in the Voynich manuscript using spectral analysis. Cryptologia, v. 25, n. 4, p. 275-295, 2001.</td>
</tr>

<tr>
<td bgcolor=#33cc33>Clarissa Simoyama David</td>
<td> É comum tratarmos sobre palavras e stopwords quando falamos de normalização de texto. Revisando a aula de expressões regulares (ER), quando possuímos uma sub- expressão entre parênteses em uma ER, podem ser de dois grupos: grupo 0 em que há o casamento inteiro e grupo 1 em que há o primeiro casamento entre parênteses, da esquerda para a direita. Às vezes em ER não é possível pegar palavras que contenham símbolos como acentos, mas que seriam de suma importância. Nestes casos há a necessidade da utilização da tabela ASCII estendida, com exceção de alguns símbolos que não são utilizados na escrita. Quando lidamos com palavras em PLN falamos de um termo chamado tokenização, que consiste na tarefa de identificar tokens, isto é, palavras em um dado documento ou texto. Também é utilizado o termo corpus, uma coletânea de documentos ou sentenças sobre um determinado assunto, utilizados para aprendizado (análise) e validação. Há evidências de que quanto maior o vocabulário maior é o número de tokens, representando um crescimento de um vocabulário. Vimos a parte prática em python sobre identificação de palavras e a quantidade de vezes que aparecem em um documento. Nem sempre a tokenização é uma tarefa fácil, e muitas vezes pode ser bem complexa, como é o caso em do teste realizado em aula que dava frases e pedia a quantidade de palavras. Além disso, em idiomas que possuem símbolos incorporados, como kanji (ideogramas) em linguagens como japonês e chinês, é necessário a utilização de outro tipo de algoritmo para a contagem de frequências. O mais comum é o maximum mathing que realiza uma busca gulosa na frase para a identificação da separação das palavras, sendo necessário um dicionário, mas não funcionando muito bem para línguas como o inglês. Há a necessidade de pré-processamento de textos em relação à palavras que possuem poucos sentido semântico (resposta para a segunda questão do teste), também conhecidas como stopword/stoplist. Uma stopword é uma palavra que pode ser considerada irrelevante para a análise do contexto em que ela está inserida, geralmente são compostas por artigos definidos e indefinidos, preposições e conjunções. Na parte prática vimos como são feitas as leituras das stopwords e como podemos retirá-las dos textos, mas muitas vezes é compensável que o desenvolvedor crie a sua própria stoplist com suas stopwords, já que quais palavras que podem ser consideradas irrelevantes dependem do contexto em que estão inseridas. Ademais, seu uso deve ser avaliado para cada aplicação, já que algumas palavras que poderiam ser consideradas stopwords possuem grande relevância para a análise de um texto, podendo inclusive influenciar e reduzir a efetividade de uma busca, tendo muitas aplicações que não as utilizam. Podem ser realizadas também a normalização das palavras. Em línguas como o português muitas palavras são flexionadas em, por exemplo, gênero, além de inúmeros tempos distintos. Para o tratamento destas ocasiões há a normalização de palavras, isto é, a redução ou a simplificação de palavras (que fora o objetivo da terceira questão do teste). As duas técnicas mais importantes de normalização de palavras são stemming e lemmatization. A técnica de stemming consiste em reduzir uma palavra ao seu radical sem levar em conta a sua classe gramatical, sendo um processo de heurística que corta as extremidades das palavras removendo os afixos derivacionais, com um conjunto de regras dependendo da linguagem. Há diversos algoritmos de stemming, como lovins stemmer, porter stemmer e paice stemmer. A técnica de lemmatisation consiste em deflexionar palavras, retirando a conjugação verbal, alterando substantivos e adjetivos para o singular masculino, de maneira a reduzir uma palavra ao que é encontrado em um dicionário. Essas técnicas podem ser aplicadas em recuperação de informação, em que um vocabulário menor possa levar a um melhor desempenho na busca, sendo que se o objetivo é a busca é utilizado stemming, e se o objetivo é a tradução é utilizado o lemmatisation.</td>
</tr>

<tr>
<td bgcolor=#99e699>Ana da Silva de Paula</td>
<td> Para o o estudo de linguagem natural, é importante conhecer as palavras que estão presentes na língua, sua frequência e a maneira como elas se relacionam a outras palavras. Esses processos são efetuados a partir do uso de expressões regulares, sumarização e corpora. Corpora são conjuntos de textos com anotações sobre sua estrutura - referências as classes de palavras, por exemplo - que são utilizados para aprendizado e validação de sistemas de processamento de linguagem natural. Considerando a contagem de palavras, há duas métricas principais: a quantidade de tokens e o tamanho do vocabulário. O primeiro refere-se à quantidade total de palavras no texto, e o segundo à quantidade de palavras distintas. Analisando sob esse aspecto, uma pergunta surge: há relação entre essas duas medidas? Empiricamente, foi descoberto um padrão: a quantidade de vocábulos aproxima-se do produto de uma constante pela quantidade de palavras elevado a um valor entre zero e um. Tanto a constante quanto o exponencial varia de acordo com o idioma; estima-se que na língua inglesa a constante varie entre 10 e 100 e o exponencial entre 0.4 e 0.6. O gráfico dessa equação aproxima-se à de uma função de radiciação, com um crescimento íngreme nos valores de x próximos a zero e mais suave nos valores mais distantes. Isso faz sentido: um texto pequeno usa poucas palavras, mas conforme cresce precisa de muitas palavras para que informações adicionais sejam transmitidas e a redundância seja evitada. Porém, quando um texto é muito longo, mais próximo da quantidade de palavras da língua seu vocabulário se torna e a repetição de palavras torna-se inevitável, levando a quantidade de palavras diferentes a se tornar estável. Visto que essas medidas dizem respeito à quantidades de palavras, é válido questionar a maneira como as palavras são contadas. Usualmente são utilizados espaços e sinais de pontuação para separar as palavras. Entretanto, em alguns contextos isso pode não ser adequado, como quando tratamos uma palavra hifenizada, um endereço web ou textos com estruturas gramaticais mais complexas. Por essa razão, as expressões regulares devem ser escritas para um contexto específico. A extração dos tokens de um texto pode ser utilizada para descobrir a frequência de cada um de seus vocábulos. Entretanto, uma análise de frequência mostra que as palavras mais comuns consistem, principalmente, de preposições, conjunções, artigos e demais palavras que são mais importantes para a manutenção da estrutura de um texto que seu significado. Considerando isso, define-se as stopwords, que são palavras que poluem um texto e têm pouco significado, para que a análise seja mais assertiva. Tal qual as expressões regulares, a lista de stopwords deve ser feita para um propósito específico. A etapa seguinte à extração das palavras de um documento consiste em unificar palavras de significado e grafia semelhantes — por exemplo, substituir "outras", "outro" e "outrem" por algo que as represente — com o objetivo de facilitar seu processamento. Esse processo pode ser feito de duas formas: através do stemming, que transforma as palavras efetuando cortes (as palavras citadas anteriormente seriam substituídas por "outr", por exemplo), e através da lemmatization, através da qual se transforma as palavras na sua forma sem flexão, encontrada no dicionário (nela as palavras seriam substituídas por "outro"). O stemming é muito utilizado em algoritmos de busca, enquanto a lemmatization é eficiente para sistemas de tradução automática.</td>
</tr>

<tr>
<td bgcolor=#99e699>Raphael Kiyoshi Fukushima</td>
<td> Seguindo do Teste de Avaliação em Aula da aula anterior, o exercicío 6, apresenta um recurso em expressões regulares que é um tipo de memória, seguindo de lá, existem alguns detalhes que não foram esclarecidos, como qual é a ordem que os números da memória seguem em relação aos grupos, que é a ordem da esquerda para a direita em suas aparições (siga o parenteses que abre "("), ou seja, se há grupos menores dento de um grupo maior, a "memória" acessará primeiro o grupo maior e depois os parênteses da esquerda para a direita. E há o \0, que acessa toda a expressão como seu "match", ou um casamento inteiro. Sobre o problema com acentuações presentes no português, existe uma tabela ASCII estendida, ela abrange uma maior variedade de símbolos presentes nas líguas que usam o nosso alfabeto, e qualquer expressão regular (deveria) reconhecer estes, "[-'a-zA-ZÀ-ÖØ-öø-ÿ]" é a expressão utilizada para reconhecer qualquer caracter presente em uma palavra em português, por exemplo. Tokenização e Vocabulário Seria a expressão para diferenciar o número total de palavras e o número total de palavras diferentes em um texto, respectivamente. Normalmente textos tendem a possuir um padrão entre a relação entre os tokens e o vocabulário, seguindo uma curva de uma função de raiz quadrada, logo um texto que apresente um grande número de tokens e um vocabulário muito pobre ou excessivamente rico, pode ser um possível texto falso (criado automaticamente para o famoso "encher linguíça", por exemplo). No site da disciplina há alguns exemplos de código para contagem de palavras ( palavra.py), incluindo vocabulário e quantas vezes cada palavra aparece (palavra2.py). todos os códigos podem ser utilizados com o seguinte comando no terminal: python3 codigo.py texto.txt Teste de avaliação em aula + Considerações não triviais + Stopword + normalização Quais as condições ideais para se considerar onde uma palavra termina? até um ponto? até um espaço em branco? Essa não é uma pergunta fácil, e nem tem uma resposta concreta, essa é uma consideração ao se fazer um sistema de contagem de palavras, se for considerar outras línguas que usam ideogramas como chinês que não usam espaço, é necessário outro tipo de regra para definir uma palavra, nesses casos usasse um sistema chamado maximum matching que usando busca gulosa e um dicionário, ele defini o final de uma palava pela maior que ele conseguir identificar. Stopwords, diferentes de seu nome, na verdade são palavras "ignoráveis", como "a", "em", "para" em português, num sistema de relevância de palavras presentes em um texto (normalmente são padrões para certa língua, mas podem ser personalizados de acordo com seu objetivo), requerem um certo aprimoramento para certos casos e nunca/raramente são usados para ferramentas de pesquisa como google, bing, yahoo. Em certos casos uma palavra com outro gênero e/ou flexão, não é interessante, para certos objetivos, que sejam considerados como palavras diferentes, então para normalizá-las, usasse: Stemming para o menor radical possível de uma palavra utilizando regras implementadas Lemmatization para a forma mais genérica da palavra (usa dicionário)</td>
</tr>

<tr>
<td bgcolor=#99e699>Lucas Zanferrari Caraca</td>
<td> A aula iniciou-se com a revisão de utilizão da memória através de regex. Para que o resultado de um match seja armazenado na memória, coloca-se o trecho da expressão regular responsável por encontrá-lo entre parênteses. Após isto, para fazer referência ao conteúdo do primeiro espaço ocupado na memória pela regex (ou seja, o primeiro match que a expressão entre parênteses encontrou), utiliza-se a instrução \1. Analogamente, utiliza-se \2, \3, \4 e assim por diante para acessar os conteúdos seguintes encontrados e armazenados pela regex através do uso de parênteses, sempre em ordem. O \0 é uma referência ao conteúdo completo armazenado na memória pela regex. Após isso, estratégias para tratar acentuação em textos através de conhecimento prévio da tabela ASCII foram introduzidas, e o conceito de tokenização (ou seja, divisão do texto em palavas individuais) foi apresentado, assim como os desafios que esta tarefa traz consigo, por exemplo: sinais de pontuação podem ser considerados palavras? Deve-se contar palavras repetidas? A resposta costuma variar de acordo com a aplicação. O conceito de corpus foi mostrado. Corpus é um conjunto de textos ou frases, geralmente anotados, que são usados como insumo de algoritmos de aprendizado e validação dos resultados apresentados por estes algoritmos. A análise empírica de textos nos fez chegar a conclusão que o vocabulário presente em um documento é diretamente proporcional (mensurado através da Lei de Herdan, em 1960, e da Lei de Heaps, em 1978) ao número de palavras nele contidas. Vocabulário é definido como o número de palavras diferentes no documento, enquanto tokens são todas as palavras presentes (incluindo as repetidas). Para efetuar tokenização em textos compostos por caracteres não presentes na tabela ASCII (como textos baseados em kanjis, por exemplo), existe um algoritmo chamado Maximum Mathing, mas ele é caro, utiliza busca gulosa, requer um dicionário e não funciona para textos em língua inglesa ou similares. Para tal, usa-se algoritmos mais complexos baseados em modelos estatísticos e técnicas de aprendizado supervisionado. Stopwords - ou stoplist - foram definidas como quaisquer palavras que não sejam significativas para o contexto da análise a ser efetuada. Normalmente, tratam- se de conjunções, artigos, conectivos, preposições etc. Por fim, a noção de normalização de palavras foi mencionada. Ela consiste em encontrar representantes para grupos de palavras flexionadas em gênero, número e/ou grau, para que estas sejam selecionadas em conjunto ou contabilizadas apenas uma vez, por exemplo. Duas técnicas populares para normalização de palavras são Stemming e Lemmatization. A primeira consiste em reduzir palavras as suas raizes: a raiz de "outros", "outro", "outras", "outras", "outrem" pode ser "outr", por exemplo. Neste caso, raizes são prefixos em comum entre todas as formas de flexão de uma palavra, mas existem diversas heurísticas que podem ser utilizadas (assim dando origem a outros algoritmos). É preferível de ser aplicada em buscas. A segunda gera um dicionário de palavras e possui heurística mais sofisticada. Normalmente é utilizada em traduções. </td>
</tr>

<tr>
<td bgcolor=#99e699>Jair Pereira Junior</td>
<td> Aula 3. Grupos refere-se ao uso de parênteses em uma expressão regular. A numeração dos grupos é feita da esquerda para a direita. Grupo 0 é quando não há parênteses causando casamento inteiro da RE. Grupo 1 é o casamento da RE dentro do primeiro parênteses. De modo geral, Grupo N é o casamento da RE dentro do N-ésimo parênteses. Token é uma palavra ou um outro elemento atômico capaz de ser analisado, isto é, não foi ignorada pela RE. Vocabulário é o número de tokens diferentes no texto. Corpus, em NLP, é um conjunto de documentos utilizados para aprendizado (análise) e validação (verificação). A lei de Herdan, também conhecida como lei de Heaps, é uma lei determinada empiricamente que descreve o tamanho do vocabulário em um documento ou um conjunto de documentos em função do número de tokens no documento. É descrita por |V| = kN^beta, onde V é o vocabulário, N é  o número de tokens, k e beta são parâmetros determinados empiricamente sendo 0&lt;beta&lt;1. Problemas com RE. Em línguas de alfabeto latina, mas com modificadores, o shortcut \w ignora tais caracteres. A definição do que é um token pode depender da aplicação, mas de modo geral está atrelado ao significado semântico. Por exemplo, 'R$10,45' deve ser entendido como um token apenas. Outro exemplo, 'L'ensemble' também deve ser entendido como um token apenas. Deste modo, observamos que o mecanismo de RE pode funcionar relativamente bem para o inglês. No entanto, RE não funcionam tão bem dependendo do quanto mais distante a língua for do inglês. Tal problema agrava-se quando notamos que RE depende de espaços entre os tokens para funcionar bem. No entanto em línguas como japonês, chinês e outras, não há espaço entre palavras. Sendo necessário o uso de outros métodos, como o algoritmo guloso Maximum Matching, requerendo um dicionário. Outros métodos mais sofisticados usam modelos estatísticos (aprendizado de máquina). Stopwords são palavras que podem ser consideradas irrelevantes para a análise, como por exemplo preposições. No entanto, cada aplicação deve tomar cuidado na escolha de suas stopwords. Ao analisar uma obra literária, provavelmente poderíamos incluir o artigo 'a' na lista de stopwords. Mas se usarmos a mesma lista de stopwords para analisar um livro de biologia, teríamos o problema de ignorar o token 'vitamina A', que poderia ser recorrente. O artigo "When stopword lists make the difference", dos autores Ljiljana Dolamic & Jacques Savoy sugere uma lista de stopwords mínima, de aproximadamente 9 tokens. Normalização de palavras é a simplificação de palavras de modo que palavras de mesmo valor semântico, mas que possuem inflexões, sejam analisadas da mesma forma. Existem duas técnicas principais para normalizar palavras, Stemming e Lemmatization. Stemming ignora a classe gramatical reduzindo a palavra à sua raiz. Lemmatization aplica regras para transforma a palavra em sua forma de dicionário. Ambos métodos dependem da aplicação. Por exemplo, Stemming é mais útil em buscas enquanto Lemmatization é mais útil em traduções.</td>
</tr>

<tr>
<td bgcolor=#99e699>Wedeueis Braz da Silva</td>
<td> Normalização de texto: Palavras e stopwords. Bibliografia: “Speech and Language Processing: An introduction to natural language processing, computational linguistics, and speech recognition.”, por Daniel Jurafsky e James H. Martin, Pearson/Prentice Hall. Parte contida no capítulo dois da bibliografia. Da aula anterior vimos como recuperar todas as palavras que aparecem em duplas, ou seja, repetidas duas vezes consecutivamente, usando o seguinte comando de expressões regulares: \b(\w+) \W\1 Em expressões regulares cada sub-expressão entre parenteses é um grupo. Os grupos são numerados da esquerda para direita. Ex: ((\w+)\.(\w_+))@\1 ? a expressão irá buscar uma sequencia de pelo menos uma palavra, seguida de um ponto, seguida de outra sequencia de pelo menos uma palavra, então um arroba seguido do resultado de “\1”, que significa o grupo da busca anterior de número 1, ou seja tudo que está dentro dos parênteses mais externos, (\w+)\.(\w_+) . Se fosse \2, buscariamos o conteudo encontrado pelo grupo do primeiro parênteses interno, (\w+). Palavras (tokenização) O comando \w+ para buscar palavras de ER sginifica [a-zA-Z]+ portanto não irá encontrar palavras que possuam acentuação. Para isso devemos usar a expressão [-'a-zA-ZÀ-ÖØ-öø-ÿ]. token: é um conjunto de caracteres (de um alfabeto, por exemplo) com um significado coletivo. O processo de tokenização recebe um texto e retorna sua decomposição em uma lista ligada de tokens. Na frase “Muito longo para os que lamentam, muito curto para os que festejam”temos 12 tokens, mas somente 8 palavras diferentes(em termos de tipo/ vocabulário). Em PLN um Corpus textualé um conjunto de documentos ou de frases, geralmente anotados, utilizados para aprendizado(análise) ou validação(verificação). * Foram considerados alguns exemplos práticos de programas em Python. Stopwords sãopalavras com pouco sentido semântico, em muitos contextos pode ser considerada irrelevante para análise (artigo, preposição). Podem ser eliminadas antes ou depois do processamento, mas deve ser avaliada sua importância para cada aplicação. Normalização de palavras Em textos em português temos diferentes palavras flexionadas em gênero, número, grau, intensidade etc., além de inúmeros tempos verbais distintos. Normalização de palavras pode ser entendido como o processo de simplificação de palavras. Existem duas ténicas principais: Stemming, que consiste em reduzir as palavras à sua raiz, sem considerar sua classe gramatical. Geralmente refere-se a um processo de heurística que corta as extremidades das palavras inclui frequentemente a remoção de afixos derivacionais, pode ser representado por um conjunto de regras que dependem da linguagem. Ex: amig ? amigo, amiga, amigão, amigos… Lemmatisation,consiste aplicar uma técnica para deflexionar as palavras (retira a conjugação verbal, caso seja um verbo, e altera os substantivos e os adjetivos para o singular masculino, de maneira a reduzir a palavra até sua forma de dicionário). Geralmente usa um dicionário de palavras (a heurística é mais sofisticada). As operações de stemming e lemmatization são operações comuns e iniciais utilizadas em sistemas de recuperação de informação. </td>
</tr>

<tr>
<td bgcolor=#99e699>Leandro Akira Tochiro</td>
<td> No início da aula, revimos o conceito de "memória" das expressões regulares, em que podemos guardar o que foi buscado dentro dos parênteses da sentença. E ainda, se tivermos um ou mais parênteses dentro de um outro, também podemos usar a mesma ferramenta para armazenar os dados. Por exemplo, no caso "((\w+)\. (\w+))@\1", a ferramenta "\1" repete o primeiro grupo, ou seja, tudo que está dentro do parênteses de fora. Um exemplo desse caso seria "alguma.coisa@alguma.coisa". Já usando "((\w+)\.(\w+))@\2", o que seria repetido seria apenas o primeiro parênteses de dentro, chamado de segundo grupo. por exemplo: "alguma.coisa@coisa". E assim por diante. O segundo desafio da aula foi determinar um método para buscar palavras que possuam hífen ou apostrofe ou acento em alguma letra. Por exemplo "homem- máquina", "d'água" ou "família". Como sabemos, usando a expressão regular "\w+", essas palavras seriam separadas e contadas como duas palavras diferentes. Para o hífen e apostrofe, a estratégia mais simples é englobá-los na busca. Por exemplo: [-'a-zA-Z]+. Contudo, para palavras com acento, seria preciso consular o alfabeto ASCII, para englobar todos os caracteres possíveis. Assim, a expressão melhor seria "[-'a-zA-ZÀ-ÖØ-öø-ÿ]". Em seguida, vimos os conceitos de "token" e "vocábulo", onde token é o número de palavras totais no texto, contando com pontuação, e vocábulos (ou tipos) são o número de tokens diferentes dentro de um texto. Vimos também que a relação entre token e vocábulo dentro de um corpus (coletânea) segue uma função do tipo raíz quadrada. Esse, inclusive, é um argumento para evidenciar se um determinado texto é ou não válido. Ou seja, é de fato um texto e não apenas um conjunto de palavras aleatórias. Por fim, acompanhamos um programa para fazer a contagem de palavras totais e contar as palavras diferentes. Concluímos que muitas das palavras mais frequentes de um texto não são significativas em relação ao assunto do texto. Por exemplo, palavras como "que, a, de, do, aquele, isso, no, na" etc. Essas palavras recebem o nome de STOPWORDS. Acompanhamos então o mesmo programa de contagem sendo rodado, porém desta vez retirando as palavras STOPWORDS. Contudo, percebemos que nem sempre as stopwords são insignificantes, e que, em alguns casos, são palavras importantes numa busca. Exemplo: Rio de Janeiro vs Rio Janeiro. No final da aula, fomos introduzidos aos conceitos de "Stemming" e "Lematization", em que o primeiro é uma ferramenta para reduzir uma palavra em sua raíz, e último consiste em transformar uma palavra em seu variante singular e masculino (como é encontrado no dicionário). Ambas tecnicas são usadas para normalizar um texto. Essa normalização é importante pois, numa análise, podemos encarar as palavras "bonito, bonitos, bonita, bonitos". Pelo Stemming, a palavra que representa melhor esse grupo de palavras é "bonit" (palavra é cortada); mas no Lematizarion, transformaríamos esse conjunto de palavras em "bonito" (singular, masculino).</td>
</tr>

<tr>
<td bgcolor=#99e699>Gabriel Martins Trettel</td>
<td> Como vimos na aula passada, podemos realizar pesquisas em texto usando expressões regulares, entretanto, na língua portuguesa temos caracteres acentuados em que sua marcação não é tão trivial. Se uma extração for feita utilizando a RE “\w+”, teríamos a separação onde tivermos acentos. Para corrigir isto, basta incluir os caracteres sequenciais da tabela ASCII no intervalo da disjunção para acrescentá-los à extração: “[-'a-zA-ZÀ-ÖØ-öø-ÿ]+”. Com isto separamos somente as palavras desejadas na nossa pesquisa,  incluindo as acentuadas. Todavia, continuamos tendo alguns problemas em definir o que é uma palavra; se a pontuação é um dos métodos de se separar o texto, “T.A.R.S.” será separado em letras individuais, perdendo o sentido original. Outro problema clássico é em relação a números com casas decimais ou então palavras que incluem caracteres especiais em sua composição, como notação monetária. Para estes casos é necessário técnicas mais avançadas para entender onde a pontuação realmente termina uma palavra e quando ela faz parte dela. Levando em conta que já temos todos os problemas de separar palavras válidas sob controle, podemos chamar cada uma delas de “token”. Neste ponto, se calculada a frequência de cada token no texto, vemos que, independente da entrada, teremos sempre as mesmas classes de palavras como sendo as mais frequentes; artigos, preposições e advérbios. De forma geral, estas palavras com alto grau de repetição não carregam muita informação se analisadas de forma isolada, podendo ser descartadas(dependendo do caso). Chamamos esses tokens “descartáveis” de stopwords ou stoplist. Stopwords comuns em português podem ser “o, a, então, é, portanto”, dentre outros. Depois de removidas todas as stopwords, ainda temos várias palavras repetidas, mas em flexões diferentes. No fundo, as palavras “aluno, aluna, alunos, alunas” são todas as mesmas, da mesma forma que todas as conjugações do mesmo verbo trazem a mesma informação individual. O objetivo agora é modificar, normatizar, cada uma das flexões para que todas estejam escritas da mesma forma. Para isto existem dois métodos principais, o Stemming e o Lemmatization. No stemming consideramos somente a parte que não muda na palavra, ou seja, “aluno, aluna, alunos, alunas” seriam substituídos por “alun”, que é a parte invariável. Já o método Lemmatization, reduz a palavra para a sua forma não conjugada, ou no masculino e singular. Assim, “aluno, aluna, alunos, alunas” seriam substituídos por “aluno”. É importante ressaltar que as técnicas de remover as stopwords e depois normalizar o texto variam de acordo com a utilização. Em estudos relacionados à análise de sentimento, não podemos cortar certas stopwords, já que elas são cruciais para entender a real intenção do escritor. Em contrapartida, alguns motores de busca efetuam estes métodos para reduzir a quantidade de buscas no banco de dados, agilizando a pesquisa do usuário. ou seja, cada aplicação deverá receber uma abordagem diferente, corroborando com o objetivo final.</td>
</tr>

<tr>
<td bgcolor=#99e699>Gustavo Tino Ferreira</td>
<td> O uso de parênteses ((\w), por exemplo) indica um agrupamento, que serve como um princípio de memória nas ER, pois é possível ser referenciado através de “\1”. Porém, caso tenham parênteses dentro de parênteses, como em “((\d+)\. (\w+))”, é possível referênciar cada um dos agrupamentos, sendo lido da esquerda para a direita. Então “\2” indica o grupo (\d+), e assim por diante. Para acentos ou caracteres especiais, consultar a tabela ASCII estendida, todos os símbolos podem ser usados nas Ers. É nececssário diferenciar token de vocabulário (ou tipo). O primeiro consiste em qualquer palavra, ou cadeia de caracteres que possuam caracteres de separação antes e depois (vírgulas, pontos e espaços). Então, quando falamos de tokenizção, estamos separando as palavras de um texto, mesmo que repetidas. Já um vocabulário/tipo, consiste apenas de palavras diferentes. Por exemplo, a frase “Fiz um bolo, comi um bolo” possui 6 tokens e 4 vocabulários. Corpus Textual é um conjunto de documentos de texto utilizados para Aprendizado e validação. Por exemplo, a análise empírica que gerou a Lei de Herdan/Heaps, que diz que o crescimento de um vocabulário em relação ao de tokens é da ordem de: |V| = kN^b, onde 0 &lt; b &lt; 1. Isso é muito últil para analise de texto criptografados ou com linguagens perdidas. No caso do manuscrito Voynich, não sabiamos se era um texto com uma linguagem humana ou um texto com caracteres aleatórios e sem sentido. Como o texto obedece à Lei empírica de Herdan/Heaps, podemos considerá-lo um texto real. Uma outra forma de segmentar um texto é usando o algoritmo Maximum Matching, que usa modelos estatísticos (aprendizado supervisionado) e um dicionário. Para uma análise melhor de um texto após a segmentação é necessário usar Stopwords, que são palabras muito usadas na maioria dos textos (sempre ocupando as primeiras posições num ranking de ocorrências) e, no contexto de processamento e análise, não possuem significado semântico. Po?em, antes de usar uma banco de stopwords, é necessário analisar a situalção e adicionar exceções, pois em alguns contextos algumas das stopwords podem ter um significado grande (por exemplo Internet das coisas viraria Iternet coisas, perdendo grande parte do seu sentido). É necessário, em alguns casos, normalizar palavras. Por exemplo, comeriam e comerão podem ser consideradas uma palavra só (devido ao seu sentido) e poderiam ser reduzidas a comer. Esse processo de simplificação pode ser feito através de Stemming, que reduz uma palavra à sua raiz (amiga e amigos viram amig). Isso é feito através de um corte do final da palavra, dado por uma heurística (conjunto de regras). Ou ainda através de Lemmatisation, que usa um dicionário de palavras (menos eficiência na memória) para reduzir diversas variações de uma mesma palavra (amiga, amigas e amigos viram amigo, que é a forma mais resumida).</td>
</tr>

<tr>
<td bgcolor=#99e699>Hiago Lucas Cardeal de Melo Silva</td>
<td> Começamos a aula fazendo a definição de alguns termos. Tokens) Quantidade de palavras existentes em um texto. Tipo/Vocábulo: Quantidade de palavras diferentes no texto. Corpus) Coletânea sobre um determinado assunto geralmente utilizado para aprendizado e validação. Em geral existe uma relação de lei de potência entre o número de vocábulos |V| e a quantidade de tokens N: |V| = kN^(B), onde 0&lt;B&lt;1. O interessante desta relação é que através dela é possivel ter indícios se o texto em questão se trata de fato de uma linguagem. Após esta introdução, o professor nos ofereceu um teste cotendo 3 questões: 1) Havia diversas palavras contendo caracteres e tínhamos que dizer quantas palavras haviam cada linha. Existiam exemplos simples como a palavra "total" e outros mais relativos como o termo www.ufabc.edu.br. 2) Explique o que é stopword. Inicialmente não poderíamos responder esta pergunta uma vez que o conceito de stopword ainda não havia sido apresentado. Mais tarde, descobrimos que stopwords se tratam de palavras com pouco significado semântico no texto e que, portanto, não contribuem para a identificação de seu tema. 3)  A pergunta pedia para indicar uma palavra que melhor representa-se as outras. Dado as palavras "outra", "outro", "outras" e "outros", encontre uma palavra que melhor representa o conjunto. Este caso foi mais simples. A palavra "outr" é uma boa candidata pois está contida em todas as outras palavras. No entanto, o segundo caso era mais complicado. O conjunto de palavras era "tinha", "tenho", "tem", "ter". Este caso é mais complexo que o anterior pois as palavras não tem uma estrutura base. O professor disse que veríamos técnicas para estes casos mais complexos posteriormente. Após o teste nos aprofundamos no conceito de stopwords. Vimos, que em geral elas são as palavras mais frequentes de um texto. Também foi nos apresentado um exemplo de um texto onde todas as palavras foram removidas, exceto as stopwords. Era impossível dizer sobre o que se tratava o texto, contribuindo para a ideia de que stopwords não carregam grande significado semântico. Embora em muitos casos as stopwrods sejam desnecessárias, devemos tomar cuidado com alguns casos específicos. A palavra "A" é uma das palavras mais frequentes do idioma português e é uma stopword, no entanto, na expressão "Vitamina A", ela representa um papel semântico muito imporatante. Portanto, é imprescindível analisar quais serão as stopwords que removidas do texto. Finalmente, vimos o conceito de normalização de palavras: O idioma português possui muitas palavras flexionadas em gênero, número ou grau. A normalização tem como objetivo reduzir ou simplificar estas palavras. Vimos dois métodos de normalização: 1) Stemming: consiste em reduzir a palavra em sua raiz. 2) Lemmatisation: consiste em deflexionar a palavra para sua forma no dicionário.</td>
</tr>

<tr>
<td bgcolor=#99e699>Andre Oliveira Macedo</td>
<td> Grupos: É a sub-expressão de uma expressão regular (er) entre parênteses. Tokenização lexical: É o processo de separar cada palavra como um token, isto é, uma unidade, mesmo que junta a pontuações. Maximum Matching: É um algoritmo guloso de segmentação de palavras, para obter a maior palavra no dicionário que corresponde aos caracteres que começam a sentença. Pode funcionar bem em algumas línguas, mas existem modelos estatísticos mais sofisticados. Stopwords: A remoção de stopwords consiste em retirar palavras muito frequentes, como artigos, que não adicionam informações relevantes ao documento. Corpus textual: Conjunto de documentos que serve de base para aprendizado (análise) e validação(verificação). Normalização de palavras: Como as palavras possuem diferentes formas, como seu gênero, número e grau, é necessário uma maneira de reduzi-las ou simplificá-las para uma forma padronizada ou única. Tais técnicas são muito importantes na extração de informação, sumarização e classificação de textos. Nesses casos, o conceito utilizado no texto é mais relevante do que o item lexical que vincula a ideia. Assim, a normalização morfológica pode ser vista como o princípio que relaciona itens que vinculam o mesmo conceito de significado. Stemming: Redução de palavras flexionadas ou derivadas, a sua base ou raiz, geralmente utilizando alguma heurística para cortar as extremidades, removendo afixos derivacionais ou utilizando um conjunto de regras. Geralmente é suficiente que as palavras sejam relacionadas no mesmo grupo para possuírem uma raíz válida. Assim, é possível que um motor de busca utilize palavras com a mesma origem como sinônimos para realizar uma consulta expandida, combinando mais termos e realizando assim uma busca mais profunda, aumentando a taxa de positivos verdadeiros, pois são relacionados ao tema procurado, porém, reduz a precisão. Lemmatization: São técnicas utilizadas para reduzir a palavra ao seu lema ou sua forma de dicionário (canônica), deflexionando as palavras, dependendo da língua utilizada, no caso do português retira-se a conjugação verbal e adequa- se os substantivos e adjetivos para a forma singular masculina. Além disso, lemmatization precisa identificar corretamente o significado da palavra na sentença, como parte do contexto no documento como um todo ,se tornando um problema extremamente complexo. Ao traduzir uma palavra de outra língua, é importante entender a interpretação dada e o contexto, não somente a palavra de origem, que pode não significar muito. Como por exemplo, em português ao utilizar o termo "pé-frio" que é sem sorte em português, o termo "cold feet" em inglês está mais relacionado a uma pessoa medrosa ou perda de confiança.</td>
</tr>

<tr>
<td bgcolor=#99e699>Eduardo da Silva Cruz</td>
<td> Resumo  Foi realizada uma revisão da aula anterior abordando tópicos de expressões regulares. Foram mostrados comandos para selecionar duas palavras iguais consecutivas através do comando \b(\w+)\W+\1 Sendo que: Cada sub-expressão entre parênteses em uma ER é um grupo: \W+ é uma expressão regulara para filtrar palavras A tabela ASCII estendida foi abordada para caracteres especiais. Na sequência foi apresentando o conceito de token e "tokenização" para palavras, abrangendo palavras únicas e questões de pontuação. No contexto de PLN, um corpus é um conjunto de documentos (ou de frases) geralmente anotados e utilizados para: Aprendizado (análise) Validação (verificação) Em www.corpusdoportugues.org é possível verificar um  Corpus do Português que tem duas partes distintas: um corpus (original e menor) que permite ver as mudanças históricas assim como variações de género. um corpus (novo e muito maior) que permite verificar as variações dialéticas (e tem 50 vezes mais dados do português moderno) Foram apresentadas aplicações do conteúdo visto na aula na parte pratica no texto "A semana" de crônicas reunidas de Machado de Assis. Algumas considerações não triviais foram abordadas tomando como exemplo textos e frases que possuem dígitos numéricos, caracteres especiais e acentuação, como por exemplo: Total de R$10,45 Para valores superiores a 45.455,67 www.ufabc.edu.br UFABC Livre-docente Homem-Maquina D'água U.F.A.B.C. C.M.C.C. Ph.D. Sant'Anna L'ensemble Lebensversicherungsgesellschaftsangestellter ??????????????????? O algoritmo Maximum Mathing que utiliza busca gulosa, pode ser usado para separar palavras em textos que tenham sido elaborados sem a utilização do espaço. Uma stopword é uma palavra irrelevante para a análise do texto (ex: a ao aos aquela aquelas aquele aqueles aqui aquilo...) .Uma stoplist é uma lista de stopwords O conceito de normalização das palavras foi iniciado com a introdução de duas tecnicas importantes : Stemming: O processo de stemming consiste em reduzir a palavra à sua raiz (sem levar em conta a classe gramatical). Stemming geralmente refere-se a um processo de heurística que corta as extremidades das palavras inclui frequentemente a remoção de afixos derivacionais.  Ex.: amig : amigo, amiga, amigão gat : gato, gata, gatos, gatas Lemmatization: O processo de lemmatisation consiste aplicar uma técnicas para deflexionar as palavras (retira a conjugação verbal, caso seja um verbo, e altera os substantivos e os adjetivos para o singular masculino, de maneira a reduzir a palavra até sua forma de dicionário). Ex.: amigo : amigo, amiga, amigão gato : gato, gata, gatos, gatas ter : tinha, tenho, tiver, tem</td>
</tr>

<tr>
<td bgcolor=#99e699>Marcos Seiti Suzuki</td>
<td> Normalização de texto: Palavras (tokens) e stopwords Os tokens são os itens presentes em um texto ou palavras. Conforme a forma de que se deseja trabalhar com o texto sinais de pontuação por exemplo podem ser considerados tokens. Uma coleção textual pré-determinada para ser utilizado em treinamento de máquina, validação são chamados de Corpus (plural de corpora). Apresentam assuntos diversificados, geralmente em formato eletrônica podendo ter conteúdo de linguagem escrita ou falada. Uma coleção textual em português brasileiro que podemos citar são as obras escritas por Machado de Assis disponível gratuitamente pela CAPES. Vocabulário: consiste na quantidade de palavras presentes em um texto, desconsiderando suas repetições. A lei de Herdan (Herdan, 1960) ou Lei de Heaps (Heaps, 1978), encontraram uma relação empírica entre os tokens e vocabulários em uma corpora, descrita pela equação: |V|=kN^B V é o tamanho do vocabulário; N é o tamanho de token; k e B são parâmetros positivos, B depende do tamanho do corpus e do gênero. Stopwords ou stoplists são palavras com pouco sentido semântico, geralmente são artigos, preposições, etc. Muitas vezes os stopwords não são utilizados pois podem atrapalhar no resultado da analise desejada, deixando as palavras sem ou perdendo contexto, perda de semântica. O estudo de stopwords remonta desde 1960. Normalização é a redução ou simplificação das palavras, de modo a agrupar em classes ou agrupamentos que os representem. Duas técnicas importantes são: Stemming e Lemmatization Antes do processo de normalização é feito a segmentação da palavra ou Tokenização. Em algumas línguas como português as palavras já estão naturalmente segmentadas. Já em línguas como como chinês e japonês a segmentação é feito pelo máximo casamento ou MaxMatch, onde a palavra é comparada com um dicionário da língua caso seja encontrado essa palavra ela é agrupada caso contrário é retirado um caractere e feito novamente a verificação até ocorrer o casamento. O Stemming é uma heurística que realiza o truncamento da palavra até sua raiz, como por exemplo eliminar prefixos e sufixos, remover letras indicadoras de gênero, grau e numero. Exemplo: outro, outras, outros, outra poderia ser simplificado para "outr" Lemmanization ele classifica palavras em um mesmo grupo morfológico, no português seria a deflexão dos verbos, em outras como alemão ou russo ocorre para substantivos, adjetivos, pronomes. Exemplo: tinha, tenho, tem, ter podem ser representados somente pela palavra "ser" como simplificação. O Stemming geralmente é usado para buscas e o Lemmanization para traduções.</td>
</tr>

<tr>
<td bgcolor=#99e699>Lucas Theodoro Guimaraes de Almeida</td>
<td>  A aula começou retomando alguns conceitos da segunda aula, como a idéia de expressão regular, a memória existente por parênteses, após isso foi definido a representação de palavra, já que /w+ não atinge todas as palavras do português, assim usaremos [-’a-zA-ZÀØ-Ö-öø-ÿ]+ para selecionar todas as palavras em português, vale ressaltar que as vezes um ponto no meio da palavra não representa duas palavras, mas sempre vamos consideram que representa.  Tokens e tipo/vocabulário Tokens representam a quantidade de palavras totais ( com repetição ) enquanto tipo ou vocabulário a quantidade de palavras sem repetição ,ou seja, Tokens tem uma idéia parecida com o número de elementos de uma lista, enquanto tipo é na ideia de número de elementos de um conjunto. No parágrafo acima o tokens é igual a 47 enquanto o vocabulário é de 30, por análise empírica existe uma relação entre esses dois elementos ( tipos = k* (tokens^B, tal que k e B são números reais e B maior que 0 e menor que 1 ), é perceptível que o vocabulário aumenta com o número total de palavras, porém não de forma linear. Foi mostrado onde vamos tirar os dados para analisarmos, no caso vamos usar algumas corporas ( em uma língua corpus representa um coletivo de registros horários e/ou textos ), o nome de alguns bancos de dados são: Shakespeare, Brown corpus, Switchboard telephone conversations, COCA, Google N-grams e alguns em português disponíveis emhttps://www.corpusdoportugues.org/x.asp ehttps://www.linguateca.pt/ CETEMPublico/ ( contem corpuras com mais de 1 milhão de Tokens ).  Stop words As Stop words são palavras que contêm uma grande repetição no objeto em estudo e que não representa muito para a análise do texto, assim nos a excluimos essas da análise. Essa ferramenta tem que ser utiliza de maneira inteligente, pois seus uso em excesso nem sempre é benéfica.  Stemming e Lemmatization Streaming e Lemmatization são métodos que tentam minimizar o problema de palavras com raiz igual, mas que estão flexionadas de forma diferente, como por exemplo: homem e homens. O stemming tenta reduzir o problema cortando trechos da palavra, já a Lemmatization busca a palavra em um dicionário e encontra sua raiz, na visão computacional o stemming é mais barato computacionalmente e mais simples de ser implementado.  Python Foram mostrado 3 algoritmos em python, o primeiro mostrava todas as palavras, o segundo as 20 mais utilizadas e o terceiro a mais utilizadas com uma lista com algumas stop words. </td>
</tr>

<tr>
<td bgcolor=#99e699>Juliane Kristine de Lima</td>
<td> Ainda dentro do conceito de expressões regulares, foi abordado o tema de grupos ou agrupamentos de caracteres. Várias sub-expressões dentro de um mesmo padrão podem ser reconhecidas e numeradas, sendo que a numeração parte dos agrupamentos mais amplos para os mais restritos e segue da esquerda para a direita. Além disso, para as expressões regulares em língua portuguesa, é necessário incluir outros caracteres presentes na tabela ASCII estendida, para incluir hífen, apóstrofo e  acentuações diversas. No estudo da linguagem natural, existe o conceito de tokenização, em que cada palavra pode ser considerada como um elemento distinto (token) no texto. A padronização do que se considera token depende da necessidade do usuário, podendo ser especificada no algoritmo. O conjunto de palavras diferentes existentes no texto é definido como vocabulário, sendo este sempre menor do que o número de tokens. Verificou-se a existência de um padrão entre a relação do número de palavras em um texto e seu vocabulário, presente em qualquer texto em linguagem natural. Outro conceito é o de corpus textual, que se refere a um conjunto de textos, dentro de uma língua, utilizados como base de dados. Na análise de textos, alguns algoritmos podem ser implementados buscando a compreensão textual. Um deles é o algoritmo de casamento máximo de palavras, que é mais eficiente em algumas línguas e não em outras. Neste caso, a partir de um dicionário pré-estabelecido busca-se identificar palavras (as maiores possíveis - greedy) a partir de um texto corrido, sem espaços. Outro conceito é o de stopwords, que são palavras irrelevantes para a análise  textual, incluindo artigos, preposições e outras palavras muito utilizadas que pouco contribuem para o significado do texto. A exclusão das stopwords é útil, porém deve ser feita com cuidados específicos, que dependem de cada tipo de texto. Também utiliza-se a normalização de palavras, que é a busca de reduzir várias palavras com o mesmo significado semântico para um único token. Na técnica de stemming, busca-se reduzir as palavras à sua raiz comum (gat representaria gato, gata, gatos, etc.). Já na técnica de lemmatization, busca-se transformar todas as palavras do grupo para uma palavra genérica, por exemplo, substantivos mudam para a versão no singular e masculino e verbos mudam para o infinitivo. Na prática da criação de algoritmos, Python é uma das linguagens que melhor se adequa ao processamento de linguagem natural, pois já tem funções específicas para esse tipo de trabalho.    </td>
</tr>

<tr>
<td bgcolor=#99e699>Lucas Mazim de Sousa</td>
<td> Na terceira aula de PLN o professor falou sobre normalização de texto. Como vem acontecendo, os slides são baseados no livro Speech and language processing. Revimos o exercício 6 da aula passada, em que deveríamos encontrar um padrão no texto em que palavras se repetissem, como em "bola bola", "casa casa". A ER para isso é \b(\w+)\W+\1. \b é uma ancora que indica se queremos encontrar palavras que contenham o padrao (apenas para letras) no inicio ou fim da palavra, dependendo de onde esta o \b. \1 indica que procuramos um padrão (subgrupo) definido por ( ) que se repete. É importante reparar que vogais com acento geralmente nao sao capturadas por expressoes regulares por serem considerados simbolos distintos pela tabela ASC II. Entao, antes de processar o texto é importante realizar um tratamento retirando acentos e deixar o texto em um mesmo formato de letras (maiusculas ou minuscuilas). Nesse contexto, tokens remete ao número de palavras existentes em um documento enquanto vocabulario é o conjunto de palavras distintas de determinado documento. Um conjunto de documentos é chamado de corpus, geralmente utilizamos eles para fazer analise ou validação. A Lei de Herdan tenta descrever empiricamente a relação entre o número de palavras distintas de um documento em função do seu comprimento. Ela é descrita como |V| = k(n^B). Na parte prática, trabalhamos com o pacote sys (fornece informações sobre constantes, funções e métodos do interpretador python) e o pacote re(modulo que fornece expressoes regulares no python). Analisamos um corpus formado por cronicas do machado de assis. Stopwords são palavras consideradas irrelevantes em um contexto, muitas vezes retiramos ela do corpus para nao atrapalhar a analise, porém, o tamanho de uma lista de stopwords pode reduzir a efetividade de uma busca. Normalizar uma palavra é o mesmo que reduzila ao seu radical, existem duas tecnicas para fazer isso: Steamming (reduzir a palavra ao seu radical; corta as extremidades da palavra; pode ser representado como um conjunto de regras que depende da linguagem; sample text, lovin steammers, porter steamers, paice steamers) e Lemmatization (consiste em aplicar técnicas para desflexionar as palavras. Retira conjugação verbal, altera substantivos e adjetivos para o singular masculino; reduz a palavra até sua forma de dicionario; heuristica é mais sofisticada, utiliza um dicionario de palavras). utilizamos streaming quando queremos realizar busca e lemmatization para tarefas de tradução.</td>
</tr>

<tr>
<td bgcolor=#99e699>Paulo Ricardo Cunha da Silva</td>
<td> Descrevemos a aplicação da tecnologia de processamento de linguagem natural (PNL) para a análise de linguagem subjetiva. Em particular, nos concentramos no problema da classificação de opinião de material textual extraído de fontes de dados relacionadas a negócios. Usamos recursos baseados em palavras e sentimentos para induzir um classificador com base no uso de Máquinas de Vetor, obtendo resultados de última geração. A partir disso é também possível que experiências preliminares onde o uso de resumos antes da classificação de opinião oferece vantagem competitiva sobre o uso de documentos completos quando os documentos são longos e contêm material tanto subjetivo como não subjetivo. O conceito de Token também é muito útil pois dependendo de como é empregado uma frase pode ter mais ou menos e assim mudar a forma como a mineração do texto é feita. A pesquisa nesta área desencadeou graças a iniciativas de avaliação das Conferências de Recuperação de Informação (TREC) (Ounis et al., 2008) com a tarefa de classificação de opiniões, a Conferência de Análise de Texto (TAC) de geração de Resumos do Texto2 de opiniões e do programa de "Defi Fouille de Textes" interessado na classificação de opinião (Grouin et al., 2009). O conceito de StopWord também foi explicado onde algumas palavras extremamente comuns que parecem ter pouco valor para ajudar a selecionar documentos que correspondem a uma necessidade do usuário são excluídas do vocabulário inteiramente. Essas palavras são chamadas de StopWords. A estratégia geral para determinar uma lista de parada é classificar os termos pela frequência de coleta (o número total de vezes que cada termo aparece na coleção de documentos) e, em seguida, tomar os termos mais frequentes, muitas vezes filtrados à mão para o seu conteúdo semântico em relação ao o domínio dos documentos sendo indexados, como uma lista de parada, cujos membros são então descartados durante a indexação. Cabe notar que tivemos revisão da aula passada acerca de Expressões Regulares contendo a última questão do teste anterior aplicada e novamente um novo teste foi empregado ao final da aula para fixação do conhecimento da classe.</td>
</tr>

<tr>
<td bgcolor=#99e699>Marcos Freitas Parra</td>
<td> Primeiramente, vimos sobre a última atividade onde usamos \1 para referenciar a expressão entre parênteses, onde temos algo parecido com uma memória, o número após a barra invertida é o número do parênteses, de 1 a n parênteses. Cada subexpressão dentro de um parênteses nós chamamos de grupo, onde uma expressão pode ter de 1 a n grupos de subexpressão, sendo a ordem contada da esquerda para a direita. O grupo 0 é reservado para a expressão inteira, independentemente de parênteses. Um problema que temos com PLN quando analisamos o português, é o fato de na tabela ASCII padrão não existir um código hexadecimal para as acentuações, com isso expressões como \w+ não funcionarão com acentos, identificando apenas parte da palavra. Para a solução deste problema, podemos usar a tabela ASCII estendida, que, diferente da ASCII padrão que abrange de 0 a 127(7F hexadecimal), ela abrange de 0 a 255(FF hexadecimal)[http:// www.theasciicode.com.ar/]. Tivemos também a definição de tokenização e vocabulário, onde tokenização se refere a todas as palavras de um texto e vocabulário a todas as palavras diferentes em um texto. Vimos também, por um exemplo de uma obra de Machado de Assis, que as palavras de maior frequência no texto, normalmente não são palavras chaves, e pouco se pode entender do texto pela sua análise. Algo que pode nos ajudar a encontrar as palavras chaves de um texto, é uma lista de stopword/stoplist, que ignora palavras de pouco sentido semântico. Esta lista deve ser cuidadosamente criada, normalmente, deve-se criar uma para cada análise desejada de um texto para não correr o risco de perder palavras chaves, como Rio de Janeiro, que poderia ser filtrada como Rio Janeiro, perdendo assim, a informação. Ao final da aula, vimos as técnicas de Stemming e Lemmatisation que consistem em técnicas para reduzir as palavras e encontrar palavras flexionadas.</td>
</tr>

<tr>
<td bgcolor=#99e699>Thais Larissa Batista de Andrade</td>
<td> A terceira aula de PLN abordou a normalização de texto. Normalizar um texto consiste em alterar as formas das palavras para uma que seja padrão, com o intuito de simplificá-las e facilitar a busca por palavras parecidas. Sendo que, é importante saber que palavra é um conjunto sequencial de caracteres, é a unidade da língua escrita. Podemos ou não, considerar pontos e vírgulas como palavras dependendo da finalidade da busca que queremos fazer. Para isso, chamaremos as palavras que aparecem em um texto de tokens, assim o total de tokens da seguinte frase é 9. - O rato roeu a roupa do rei de Roma E chamaremos de tipo o total de palavras diferentes em um texto. No exemplo a seguir temos o total de 8 tipos, dado que "o", "que" e "não", são contatados apenas uma vez. -  "Que não sinta o que não tenho Não lamente o que podia ter" Um conjunto de palavras, que forma um texto escrito, chama-se corpus e, uma coleção de textos ou discursos sobre um determinado assunto, chama-se corpora e eles são utilizados para o aprendizado e a validação. Podemos simplificar ainda mais nossa busca e aumentar sua efetividade se desconsiderarmos as palavras que não possuem significado relevante, como artigos e preposições. Podemos eliminar essas palavras, chamadas stopwords, antes ou depois da leitura de texto. Por fim, foram introduzidas as técnicas de normalização de textos, Stemming e Lemmatisation. Stemming, significa reduzir a palavra a sua raiz. Por exemplo, as palavras menino e menina seriam reduzidas para "menin". Essa técnica visa cortar uma extremidade da palavra, de acordo com regras gramaticais da linguagem. Já a técnica de Lemmatisation tem como objetivo, retira a conjugação verbal, e altera substantivos e adjetivos para o singular masculino. Exemplo, amigo, amigão, amiga são todas transformadas em amigo. Existem algoritmos diferentes para aplicar essas duas técnicas, sendo que é importante saber qual se adequa mais à uma determinada linguagem.</td>
</tr>

<tr>
<td bgcolor=#99e699>Maira Zabuscha de Lima</td>
<td> Para usar uma regex adequadamente na língua portuguesa, é necessário adaptar o reconhecimento de caracteres acentuados pois [a-zA-Z] não os contempla, devemos olhar na tabela ASCII estendida. A regex com caracteres acentuados é [-'a-zA- ZÀ-ÖØ-öø-ÿ]. Pontuações podem ser consideradas como palavras ou não. O documento também deve ter os caracteres normalizados para somente letras minúsculas para não haver diferenciação entre a mesma palavra em minúsculas e maiúsculas. Normalizar as letras acentuadas para suas versões sem acento pode ser necessário se a fonte de dados não tiver gramática e ortografia confiáveis, por exemplo de uma rede social. Entretanto, se a fonte de dados for gramatica e ortograficamente confiável, podemos considerar esses caracteres, exemplos: médico, medico, bebe, bebê, sábia, sabia. Corpus é um conjunto de documentos anotados utilizados para análise (aprendizado) e verificação (validação). Crescimento do vocabulário: Lei de Heaps ou Herdan V = kN?, 0 &lt; ? &lt; 1 (radiciação) O tamanho do vocabulário cresce conforme a quantidade de tokens cresce (mas não proporcionalmente). Stopword é uma palavra sem, ou com pouco, valor semântico, como artigos e preposições, exemplos: a, em, do. Normalizar as palavras do texto pode ser feito de duas maneiras. Stemming é a técnica que corta os sufixos das palavras mesmo que o resultado não seja uma palavra, exemplo: gatos, gata, gato são representados por gat. Lemmatization é a técnica que remove o sufixo ou o modifica, de modo a eliminar a conjugação de verbos, flexão de número e gênero de substantivos e adjetivos, etc. Para essa técnica é necessário um dicionário do idioma.</td>
</tr>

<tr>
<td bgcolor=#99e699>Lucas Alves Rangel</td>
<td> Algumas definições: Corpus é uma coleção de texto ou fala legível por computador. Tokens é a quantidade total de palavras em um corpus, enquanto tipos ou vocabulário é a quantidade de palavras distintas de um corpus. Em algumas línguas não possuem delimitadores entre palavras, sendo necessário processar o texto e encontrar os limites de palavras, para isso, pode ser usado o algoritmo MaxMatch, que é uma versão de busca gulosa, e precisa de um dicionário (lista de palavras da língua), o algoritmo consiste em apontar para o começo de uma string e escolhe a maior palavra no dicionário que é igual a entrada na posição atual, e o ponteiro procede para o final desta palavra na string, caso não encontre nenhuma palavra o ponteiro então avança um caracter. Este processo é então aplicado iterativamente a partir da nova posição do ponteiro. O algoritmo é usado para encontrar o limite de palavras em línguas que não usam espaços entre palavras. Em diversas línguas muitas palavras são derivações de uma outra palavra, com objetivo de expressar diferentes categorias gramaticais, como tempo, numero, grau, etc. Para algumas tarefas de processamento de linguagem natural é interessante durante o pré-processamento reduzir ou simplificar essas derivações, duas formas são: Processo de stemming: Remover de forma heurística os afixos de uma palavra para obter o seu tronco (raiz). Processo de lemmatization: análise morfológica de uma palavra para encontrar seu lema, que é uma forma normalizada de um conjunto de formas morfologicamente relacionadas, escolhidas por convenção para representar esse conjunto. Esta é a forma também é conhecida como forma de dicionário.</td>
</tr>

<tr>
<td bgcolor=#99e699>Maria Clara Vilas Boas de Souza</td>
<td> A abordagem sobre normalização de textos tratou de dois aspectos: palavras e stopwords. Para extração e/ou filtragem de termos em textos podemos usar expressões regulares, porém é necessário se atentar a expressão que está sendo feita e quais os resultados ela proporciona. Quando usada para agrupar palavras, as ERs precisam conter as características que o idioma a ser tratado contém, como comparativo podemos traçar um paralelo entre um conteúdo em inglês e outro em português, o vocabulário em português possui em certos casos acentos, caracteres especiais como traços e aspas simples, diferente do inglês.  Como regex se baseia no vocabulário da língua inglesa, a captura desses casos não é feita sem um tratamento prévio da expressão. Portanto, uma mesma expressão coletaria cadeias de caracteres diferentes em ambos os idiomas caso nada fosse acrescentado para capturar as particularidades de cada língua. Suponha o uso do shortcut ‘\w+’ para o agrupamento de palavras em um fragmento que possui o verbete “guarda-chuva”. Serão encontradas duas palavras: ‘guarda’ e ‘chuva’, o que contraria o sentido semântico do termo, que dá nome à um objeto. Para abranger casos como esse é preciso acrescentar à expressão o intervalo que possui caracteres especiais presentes no idioma. Quando tratamos de normalização podemos encontramos como uma das tarefas principais a radicalização palavras, que pode ser feita usando técnicas de stemming e lemmatization. Stemming seria a função encarregada por remover/ truncar sufixos e manter prefixos comuns. Lemmatization é o processo de agrupamento das formas inflexíveis de uma palavra para que possam ser analisadas como um único item.   </td>
</tr></table><br> <hr> Arquivo gerado por um programa.<br><p></body></html>